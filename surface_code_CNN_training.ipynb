{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Training \n",
    "\n",
    "Test CNN training vs simulated dataset size.  This uses a nearly-identical copy\n",
    "of Ulascan's code for generating the stim shots and training, just wrapped in\n",
    "some functions to make it easier to iterate over hyperparameters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-22 15:30:49.759434: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import time \n",
    "import os\n",
    "import json \n",
    "import itertools\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.optimizers import Adam\n",
    "import pymatching\n",
    "\n",
    "from circuit_generators import *\n",
    "from sampling_functions import *\n",
    "from bitpack import pack_bits, unpack_bits\n",
    "from circuit_partition import *\n",
    "from utilities_tf import *\n",
    "from CNNModel import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surface code definitions\n",
    "These are used as globals in the stim and training functions below.  (I should\n",
    "update those functions to take these SC parameters as inputs, but this works\n",
    "for now.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of worker nodes\n",
    "n_worker_nodes = 7\n",
    "\n",
    "# Surface code specifications\n",
    "d = 5\n",
    "r = 2\n",
    "kernel_size = 3\n",
    "\n",
    "# Error probabilities\n",
    "p = 0.01\n",
    "before_round_data_depolarization = p\n",
    "after_reset_flip_probability = p\n",
    "after_clifford_depolarization = p\n",
    "before_measure_flip_probability = p\n",
    "\n",
    "use_rotated_z = True\n",
    "observable_type = \"ZL\" if use_rotated_z else \"XL\"\n",
    "\n",
    "# Bit types\n",
    "binary_t = np.int8 # Could use even less if numpy allowed\n",
    "\n",
    "# Measurement index type\n",
    "idx_t = np.int8\n",
    "n_all_measurements = r*(d**2-1) + d**2\n",
    "if n_all_measurements > np.iinfo(idx_t).max:\n",
    "  idx_t = np.int16\n",
    "if n_all_measurements > np.iinfo(idx_t).max:\n",
    "  idx_t = np.int32\n",
    "if n_all_measurements > np.iinfo(idx_t).max:\n",
    "  idx_t = np.int64\n",
    "if n_all_measurements > np.iinfo(idx_t).max:\n",
    "  idx_t = np.int128\n",
    "if n_all_measurements > np.iinfo(idx_t).max:\n",
    "  idx_t = np.int256\n",
    "if n_all_measurements > np.iinfo(idx_t).max:\n",
    "  raise RuntimeError(\"idx_t is too small.\")\n",
    "\n",
    "# Call signature for circuit_partition::group_det_bits_kxk\n",
    "call_group_det_bits_kxk = lambda det_bits_dxd, data_bits_dxd=None, d=d, r=r, k=kernel_size, use_rotated_z=use_rotated_z, binary_t=binary_t, idx_t=idx_t: group_det_bits_kxk(det_bits_dxd, d, r, k, use_rotated_z, data_bits_dxd, binary_t, idx_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stim and CNN training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_measurements(measurements, d):\n",
    "  n_measurements = idx_t(measurements.shape[1])\n",
    "  # Measurements on data qubits come last\n",
    "  exclude_indices = np.array([-x-1 for x in range(d**2)], dtype=idx_t)\n",
    "  exclude_indices = exclude_indices + n_measurements\n",
    "  # Out of all measurements on data qubits, the logical qubit measurements are those on the boundary of the lattice.\n",
    "  # All other equivalent X_L/Z_L operators can be found through the combination of ancilla measurements and the chosen data qubits giving us the logical qubit.\n",
    "  exclude_indices_obsL = np.array([-x-1 for x in range(d*(d-1), d**2)], dtype=idx_t)\n",
    "  exclude_indices_obsL = exclude_indices_obsL + n_measurements\n",
    "  # From obs_bits, we want to exclude all measurements except those listed in exclude_indices_obsL\n",
    "  exclude_indices_obs = np.arange(0, n_measurements, 1, dtype=idx_t)\n",
    "  exclude_indices_obs = np.delete(exclude_indices_obs, exclude_indices_obsL)\n",
    "\n",
    "  det_bits = measurements\n",
    "  det_bits = np.delete(det_bits, exclude_indices, axis=1)\n",
    "  obs_bits = measurements\n",
    "  obs_bits = np.delete(obs_bits, exclude_indices_obs, axis=1)\n",
    "\n",
    "  data_bits = measurements[:, exclude_indices]\n",
    "\n",
    "  # Reverse the order of data_bits because exclude_indices starts from the last data qubit measurement, not the first\n",
    "  data_bits = np.flip(data_bits, axis=1)\n",
    "\n",
    "  return det_bits, obs_bits, data_bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stim_output(n_total, split):\n",
    "  # this is using globals defined in the first cell!\n",
    "  n_test = int(n_total*split)\n",
    "  n_train = int(n_total*(1-split))\n",
    "  n_samples = n_test + n_train\n",
    "  decoders = ['pymatching']\n",
    "  test_circuit = get_builtin_circuit(\n",
    "    \"surface_code:rotated_memory_\"+('z' if use_rotated_z else 'x'),\n",
    "    distance=d,\n",
    "    rounds=r,\n",
    "    before_round_data_depolarization = before_round_data_depolarization,\n",
    "    after_reset_flip_probability = after_reset_flip_probability,\n",
    "    after_clifford_depolarization = after_clifford_depolarization,\n",
    "    before_measure_flip_probability = before_measure_flip_probability\n",
    "  )\n",
    "\n",
    "  kernel_circuit_extra_depol1 = [\n",
    "    [\n",
    "      f\"DEPOLARIZE1({after_clifford_depolarization}) 13 25\",\n",
    "      f\"DEPOLARIZE1({after_clifford_depolarization}) 5 13 17 19 25\",\n",
    "      f\"#DEPOLARIZE1({after_clifford_depolarization})\",\n",
    "      f\"DEPOLARIZE1({after_clifford_depolarization}) 12 15 19\",\n",
    "    ], # parity = (1, 1)\n",
    "    [\n",
    "      f\"DEPOLARIZE1({after_clifford_depolarization}) 8 13 25\",\n",
    "      f\"DEPOLARIZE1({after_clifford_depolarization}) 5 13 17 19 25\",\n",
    "      f\"DEPOLARIZE1({after_clifford_depolarization}) 1 14 15\",\n",
    "      f\"DEPOLARIZE1({after_clifford_depolarization}) 12 14 15 19\",\n",
    "    ], # parity = (0, 1)\n",
    "    [\n",
    "      f\"DEPOLARIZE1({after_clifford_depolarization}) 8 25\",\n",
    "      f\"DEPOLARIZE1({after_clifford_depolarization}) 17 25\",\n",
    "      f\"DEPOLARIZE1({after_clifford_depolarization}) 1 14 15\",\n",
    "      f\"DEPOLARIZE1({after_clifford_depolarization}) 14 15\",\n",
    "    ], # parity = (-1, 1)\n",
    "    [\n",
    "      f\"DEPOLARIZE1({after_clifford_depolarization}) 5 13 25\",\n",
    "      f\"DEPOLARIZE1({after_clifford_depolarization}) 5 13 17 19 25\",\n",
    "      f\"DEPOLARIZE1({after_clifford_depolarization}) 2 3\",\n",
    "      f\"DEPOLARIZE1({after_clifford_depolarization}) 2 12 15 19\",\n",
    "    ], # parity = (1, 0)\n",
    "    [\n",
    "      f\"DEPOLARIZE1({after_clifford_depolarization}) 1 5 8 13 25\",\n",
    "      f\"DEPOLARIZE1({after_clifford_depolarization}) 5 13 17 19 25\",\n",
    "      f\"DEPOLARIZE1({after_clifford_depolarization}) 1 2 3 14 15\",\n",
    "      f\"DEPOLARIZE1({after_clifford_depolarization}) 2 12 14 15 19\",\n",
    "    ] # parity = (0, 0)\n",
    "  ]\n",
    "  kernel_circuits = []\n",
    "  for replace_args in kernel_circuit_extra_depol1:\n",
    "    kernel_circuit_template = \\\n",
    "    f\"\"\"\n",
    "  QUBIT_COORDS(1, 1) 1\n",
    "  QUBIT_COORDS(2, 0) 2\n",
    "  QUBIT_COORDS(3, 1) 3\n",
    "  QUBIT_COORDS(5, 1) 5\n",
    "  QUBIT_COORDS(1, 3) 8\n",
    "  QUBIT_COORDS(2, 2) 9\n",
    "  QUBIT_COORDS(3, 3) 10\n",
    "  QUBIT_COORDS(4, 2) 11\n",
    "  QUBIT_COORDS(5, 3) 12\n",
    "  QUBIT_COORDS(6, 2) 13\n",
    "  QUBIT_COORDS(0, 4) 14\n",
    "  QUBIT_COORDS(1, 5) 15\n",
    "  QUBIT_COORDS(2, 4) 16\n",
    "  QUBIT_COORDS(3, 5) 17\n",
    "  QUBIT_COORDS(4, 4) 18\n",
    "  QUBIT_COORDS(5, 5) 19\n",
    "  QUBIT_COORDS(4, 6) 25\n",
    "  R 1 3 5 8 10 12 15 17 19\n",
    "  X_ERROR({after_reset_flip_probability}) 1 3 5 8 10 12 15 17 19\n",
    "  R 2 9 11 13 14 16 18 25\n",
    "  X_ERROR({after_reset_flip_probability}) 2 9 11 13 14 16 18 25\n",
    "  TICK\n",
    "  DEPOLARIZE1({before_round_data_depolarization}) 1 3 5 8 10 12 15 17 19\n",
    "  H 2 11 16 25\n",
    "  DEPOLARIZE1({after_clifford_depolarization}) 2 11 16 25\n",
    "  TICK\n",
    "  CX 2 3 16 17 11 12 15 14 10 9 19 18\n",
    "  DEPOLARIZE2({after_clifford_depolarization}) 2 3 16 17 11 12 15 14 10 9 19 18\n",
    "  {replace_args[0]}\n",
    "  TICK\n",
    "  CX 2 1 16 15 11 10 8 14 3 9 12 18\n",
    "  DEPOLARIZE2({after_clifford_depolarization}) 2 1 16 15 11 10 8 14 3 9 12 18\n",
    "  {replace_args[1]}\n",
    "  TICK\n",
    "  CX 16 10 11 5 25 19 8 9 17 18 12 13\n",
    "  DEPOLARIZE2({after_clifford_depolarization}) 16 10 11 5 25 19 8 9 17 18 12 13\n",
    "  {replace_args[2]}\n",
    "  TICK\n",
    "  CX 16 8 11 3 25 17 1 9 10 18 5 13\n",
    "  DEPOLARIZE2({after_clifford_depolarization}) 16 8 11 3 25 17 1 9 10 18 5 13\n",
    "  {replace_args[3]}\n",
    "  TICK\n",
    "  H 2 11 16 25\n",
    "  DEPOLARIZE1({after_clifford_depolarization}) 2 11 16 25\n",
    "  TICK\n",
    "  X_ERROR({before_measure_flip_probability}) 2 9 11 13 14 16 18 25\n",
    "  MR 2 9 11 13 14 16 18 25\n",
    "  X_ERROR({after_reset_flip_probability}) 2 9 11 13 14 16 18 25\n",
    "  DETECTOR(0, 4, 0) rec[-4]\n",
    "  DETECTOR(2, 2, 0) rec[-7]\n",
    "  DETECTOR(4, 4, 0) rec[-2]\n",
    "  DETECTOR(6, 2, 0) rec[-5]\n",
    "  REPEAT {r-1} {{\n",
    "    TICK\n",
    "    DEPOLARIZE1({before_round_data_depolarization}) 1 3 5 8 10 12 15 17 19\n",
    "    H 2 11 16 25\n",
    "    DEPOLARIZE1({after_clifford_depolarization}) 2 11 16 25\n",
    "    TICK\n",
    "    CX 2 3 16 17 11 12 15 14 10 9 19 18\n",
    "    DEPOLARIZE2({after_clifford_depolarization}) 2 3 16 17 11 12 15 14 10 9 19 18\n",
    "    {replace_args[0]}\n",
    "    TICK\n",
    "    CX 2 1 16 15 11 10 8 14 3 9 12 18\n",
    "    DEPOLARIZE2({after_clifford_depolarization}) 2 1 16 15 11 10 8 14 3 9 12 18\n",
    "    {replace_args[1]}\n",
    "    TICK\n",
    "    CX 16 10 11 5 25 19 8 9 17 18 12 13\n",
    "    DEPOLARIZE2({after_clifford_depolarization}) 16 10 11 5 25 19 8 9 17 18 12 13\n",
    "    {replace_args[2]}\n",
    "    TICK\n",
    "    CX 16 8 11 3 25 17 1 9 10 18 5 13\n",
    "    DEPOLARIZE2({after_clifford_depolarization}) 16 8 11 3 25 17 1 9 10 18 5 13\n",
    "    {replace_args[3]}\n",
    "    TICK\n",
    "    H 2 11 16 25\n",
    "    DEPOLARIZE1({after_clifford_depolarization}) 2 11 16 25\n",
    "    TICK\n",
    "    X_ERROR({before_measure_flip_probability}) 2 9 11 13 14 16 18 25\n",
    "    MR 2 9 11 13 14 16 18 25\n",
    "    X_ERROR({after_reset_flip_probability}) 2 9 11 13 14 16 18 25\n",
    "    SHIFT_COORDS(0, 0, 1)\n",
    "    DETECTOR(2, 0, 0) rec[-8] rec[-16]\n",
    "    DETECTOR(2, 2, 0) rec[-7] rec[-15]\n",
    "    DETECTOR(4, 2, 0) rec[-6] rec[-14]\n",
    "    DETECTOR(6, 2, 0) rec[-5] rec[-13]\n",
    "    DETECTOR(0, 4, 0) rec[-4] rec[-12]\n",
    "    DETECTOR(2, 4, 0) rec[-3] rec[-11]\n",
    "    DETECTOR(4, 4, 0) rec[-2] rec[-10]\n",
    "    DETECTOR(4, 6, 0) rec[-1] rec[-9]\n",
    "  }}\n",
    "  X_ERROR({before_measure_flip_probability}) 1 3 5 8 10 12 15 17 19\n",
    "  M 1 3 5 8 10 12 15 17 19\n",
    "  DETECTOR(0, 4, 1) rec[-3] rec[-6] rec[-13]\n",
    "  DETECTOR(2, 2, 1) rec[-5] rec[-6] rec[-8] rec[-9] rec[-16]\n",
    "  DETECTOR(4, 4, 1) rec[-1] rec[-2] rec[-4] rec[-5] rec[-11]\n",
    "  DETECTOR(6, 2, 1) rec[-4] rec[-7] rec[-14]\n",
    "  OBSERVABLE_INCLUDE(0) rec[-7] rec[-8] rec[-9]\n",
    "    \"\"\"\n",
    "    kernel_circuits.append(stim.Circuit(kernel_circuit_template))\n",
    "\n",
    "\n",
    "  # Sampling for the dxd circuit\n",
    "  m_sampler = test_circuit.compile_sampler(seed=12345)\n",
    "  converter = test_circuit.compile_m2d_converter()\n",
    "  detector_error_model = test_circuit.detector_error_model(decompose_errors=True)\n",
    "\n",
    "  measurements = m_sampler.sample(n_samples, bit_packed=False)\n",
    "  det_evts, flips = converter.convert(measurements=measurements, separate_observables=True, bit_packed=False)\n",
    "  measurements = measurements.astype(binary_t)\n",
    "  det_evts = det_evts.astype(binary_t)\n",
    "  flips = flips.astype(binary_t)\n",
    "\n",
    "  avg_flips = np.sum(flips.reshape(-1,), dtype=np.float32)/flips.shape[0]\n",
    "  print(f\"Average flip rate for the full circuit: {avg_flips}\")\n",
    "\n",
    "  # next cell\n",
    "  n_measurements = idx_t(measurements.shape[1])\n",
    "  det_bits, obs_bits, data_bits = split_measurements(measurements, d)\n",
    "\n",
    "  # next cell\n",
    "  det_bits_kxk_all, data_bits_kxk_all, obs_bits_kxk_all, kernel_result_translation_map = call_group_det_bits_kxk(det_bits, data_bits_dxd=data_bits)\n",
    "  kernel_types = get_unique_kernel_types(kernel_size, d)\n",
    "  n_kernels = det_bits_kxk_all.shape[0]\n",
    "\n",
    "  # next cell\n",
    "  kernel_result_translation_map_f = kernel_result_translation_map[:,:,1:]\n",
    "  kernel_result_translation_map_b = kernel_result_translation_map[:,:,0:-1]\n",
    "  kernel_result_translation_det_evts = (kernel_result_translation_map_f!=kernel_result_translation_map_b).astype(binary_t)\n",
    "  kernel_result_translation_map = np.concatenate((kernel_result_translation_map, kernel_result_translation_det_evts), axis=2)\n",
    "\n",
    "  # next cell\n",
    "  det_evts_kxk_all = []\n",
    "  flips_kxk_all = []\n",
    "  converters_kernel = []\n",
    "  for kernel_circuit in kernel_circuits:\n",
    "    converters_kernel.append(kernel_circuit.compile_m2d_converter())\n",
    "  for k in range(n_kernels):\n",
    "    measurements_kxk = np.concatenate((det_bits_kxk_all[k], data_bits_kxk_all[k]), axis=1).astype(np.bool_)\n",
    "    ik = 0\n",
    "    for i, kernel_type in enumerate(kernel_types):\n",
    "      if k in kernel_type[1]:\n",
    "        ik = i\n",
    "        break\n",
    "    det_evts_kxk, flips_kxk = converters_kernel[ik].convert(measurements=measurements_kxk, separate_observables=True, bit_packed=False)\n",
    "    det_evts_kxk_all.append(det_evts_kxk)\n",
    "    flips_kxk_all.append(flips_kxk)\n",
    "  det_evts_kxk_all = np.array(det_evts_kxk_all, dtype=binary_t)\n",
    "  flips_kxk_all = np.array(flips_kxk_all, dtype=binary_t)\n",
    "  del converters_kernel\n",
    "\n",
    "  # next cell\n",
    "  class_bits = flips\n",
    "  features_det_bits = np.swapaxes(det_bits_kxk_all, 0, 1)\n",
    "  features_det_evts = np.swapaxes(det_evts_kxk_all, 0, 1)\n",
    "  features_translation_map = np.swapaxes(kernel_result_translation_map, 0, 1) # Dimensions go as [sample][kernel][cycle bits + detections (n_cycles-1)]\n",
    "  features_translation_map = np.reshape(features_translation_map, (features_translation_map.shape[0], -1))\n",
    "  features_final_det_evts = det_evts[:, -((d**2-1)//2):]\n",
    "\n",
    "  return {\"CNN\": [[features_det_bits,\n",
    "                   features_det_evts,\n",
    "                   features_translation_map,\n",
    "                   features_final_det_evts],\n",
    "                  class_bits], \n",
    "          \"pymatch\": [detector_error_model, det_evts, flips]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pymatching_result(stim_output, idxs_test):\n",
    "    detector_error_model, det_evts, flips = stim_output[\"pymatch\"]\n",
    "    pymatcher = pymatching.Matching.from_detector_error_model(detector_error_model)\n",
    "    flips_pred_pym = pymatcher.decode_batch(det_evts, bit_packed_predictions=False, bit_packed_shots=False).astype(binary_t).reshape(-1,1)\n",
    "    flips_pred_pym = pymatcher.decode_batch(det_evts[idxs_test,:], bit_packed_predictions=False, bit_packed_shots=False).astype(binary_t).reshape(-1,1)\n",
    "    pymatch_error_rate_test = np.sum(\n",
    "        (flips_pred_pym!=flips[idxs_test,:]))/flips_pred_pym.shape[0]\n",
    "    return pymatch_error_rate_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"sc-CNN-training\"\n",
    "run = \"in_memory\"\n",
    "n_totals = [10**5, 10**6, 10**7]\n",
    "frac_test = 0.2\n",
    "batch_size = 10**3  \n",
    "n_epochs = 60\n",
    "n_nodes = 150\n",
    "lrs = [0.05, 0.005, 0.0005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_params = {\n",
    "  \"d\": d,\n",
    "  \"r\": r,\n",
    "  \"kernel_size\": kernel_size,\n",
    "  \"p\": p,\n",
    "  \"before_round_data_depolarization\": before_round_data_depolarization,\n",
    "  \"after_reset_flip_probability\": after_reset_flip_probability,\n",
    "  \"after_clifford_depolarization\": after_clifford_depolarization,\n",
    "  \"before_measure_flip_probability\": before_measure_flip_probability,\n",
    "  \"use_rotated_z\": use_rotated_z,\n",
    "  \"observable_type\": observable_type,\n",
    "  \"runname\": run,\n",
    "  \"n_totals\": n_totals,\n",
    "  \"frac_test\": frac_test,\n",
    "  \"batch_size\": batch_size,\n",
    "  \"n_epochs\": n_epochs,\n",
    "  \"n_nodes\": n_nodes,\n",
    "  \"lrs\": lrs,\n",
    "}\n",
    "\n",
    "setupfile_path = f\"{base}/{run}/setup.json\"\n",
    "if not os.path.exists(setupfile_path):\n",
    "    os.makedirs(f\"{base}/{run}\", exist_ok=True)\n",
    "with open(setupfile_path, 'w') as f: json.dump(setup_params, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run stim and pymatching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average flip rate for the full circuit: 0.2298373\n",
      "PyMatching error rate for full data set: 3.1560%\n"
     ]
    }
   ],
   "source": [
    "# run stim for largest desired sample size\n",
    "max_total = np.max(n_totals)\n",
    "stim_output = get_stim_output(max_total, frac_test)\n",
    "[[features_det_bits,\n",
    "  features_det_evts,\n",
    "  features_translation_map,\n",
    "  features_final_det_evts], class_bits] = stim_output[\"CNN\"]\n",
    "\n",
    "# PyMatching error rate\n",
    "pymatch_error_rate = get_pymatching_result(\n",
    "  stim_output, np.arange(max_total, dtype=np.int32))\n",
    "flit_rate = stim_output[\"pymatch\"][2].sum()/stim_output[\"pymatch\"][2].shape[0]\n",
    "print(f\"PyMatching error rate for full data set: \"\n",
    "      f\"{100*pymatch_error_rate:0.4f}%\")\n",
    "\n",
    "# save\n",
    "pymatch_file = f\"{base}/{run}/pymatch_results.json\"\n",
    "pymatch_results = {\"flip_rate\": flit_rate, \"error_rate\": pymatch_error_rate}\n",
    "with open(pymatch_file, 'w') as f: json.dump(pymatch_results, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CNNs\n",
    "\n",
    "This trains the models defined above, and records the training history.  \n",
    "\n",
    "(I would like to save the actual models, but the built-in functions to do this\n",
    "fail here since the CNN here is built as a custom class. It seems we need to add\n",
    "code to those classes which shows Keras how to serialize the model.  That got\n",
    "complicated, so for now just save the training history and we'll need to re-train\n",
    "if we want these models for anything.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training CNN model 1/9\n",
      "    n_total = 1.0e+05 samples\n",
      "         lr = 0.05\n",
      "----------------------------\n",
      "Number of unique contributions: 13\n",
      "Total number of fractions: 28\n",
      "Total number of phases: 62\n",
      "Epoch 1/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 195ms/step - accuracy: 0.6433 - loss: 3.5800 - val_accuracy: 0.7686 - val_loss: 0.5348\n",
      "Epoch 2/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.7765 - loss: 0.5238 - val_accuracy: 0.7687 - val_loss: 0.5192\n",
      "Epoch 3/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.7725 - loss: 0.5130 - val_accuracy: 0.7687 - val_loss: 0.5042\n",
      "Epoch 4/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - accuracy: 0.7688 - loss: 0.5004 - val_accuracy: 0.7687 - val_loss: 0.4782\n",
      "Epoch 5/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.7737 - loss: 0.4705 - val_accuracy: 0.7686 - val_loss: 0.4562\n",
      "Epoch 6/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.7740 - loss: 0.4414 - val_accuracy: 0.7687 - val_loss: 0.4665\n",
      "Epoch 7/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.7746 - loss: 0.4256 - val_accuracy: 0.7686 - val_loss: 0.4150\n",
      "Epoch 8/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.7774 - loss: 0.3991 - val_accuracy: 0.7687 - val_loss: 0.4352\n",
      "Epoch 9/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.7755 - loss: 0.4027 - val_accuracy: 0.7687 - val_loss: 0.3969\n",
      "Epoch 10/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - accuracy: 0.7744 - loss: 0.3894 - val_accuracy: 0.7687 - val_loss: 0.3948\n",
      "Epoch 11/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.7766 - loss: 0.3755 - val_accuracy: 0.7686 - val_loss: 0.3888\n",
      "Epoch 12/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.7768 - loss: 0.3738 - val_accuracy: 0.7686 - val_loss: 0.3853\n",
      "Epoch 13/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.7739 - loss: 0.3813 - val_accuracy: 0.7796 - val_loss: 0.3875\n",
      "Epoch 14/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.7967 - loss: 0.3745 - val_accuracy: 0.8295 - val_loss: 0.3768\n",
      "Epoch 15/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.8310 - loss: 0.3527 - val_accuracy: 0.8133 - val_loss: 0.3706\n",
      "Epoch 16/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.8363 - loss: 0.3477 - val_accuracy: 0.8303 - val_loss: 0.3765\n",
      "Epoch 17/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.8123 - loss: 0.3655 - val_accuracy: 0.8238 - val_loss: 0.3852\n",
      "Epoch 18/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - accuracy: 0.8469 - loss: 0.3411 - val_accuracy: 0.8457 - val_loss: 0.3524\n",
      "Epoch 19/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.8542 - loss: 0.3261 - val_accuracy: 0.8566 - val_loss: 0.3417\n",
      "Epoch 20/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.8645 - loss: 0.3148 - val_accuracy: 0.8537 - val_loss: 0.3451\n",
      "Epoch 21/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.8545 - loss: 0.3242 - val_accuracy: 0.8548 - val_loss: 0.3369\n",
      "Epoch 22/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.8594 - loss: 0.3229 - val_accuracy: 0.8575 - val_loss: 0.3351\n",
      "Epoch 23/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.8577 - loss: 0.3193 - val_accuracy: 0.8584 - val_loss: 0.3412\n",
      "Epoch 24/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.8656 - loss: 0.3086 - val_accuracy: 0.8638 - val_loss: 0.3256\n",
      "Epoch 25/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.8755 - loss: 0.2957 - val_accuracy: 0.8328 - val_loss: 0.3988\n",
      "Epoch 26/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.8435 - loss: 0.3482 - val_accuracy: 0.8617 - val_loss: 0.3223\n",
      "Epoch 27/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.8719 - loss: 0.2945 - val_accuracy: 0.8670 - val_loss: 0.3206\n",
      "Epoch 28/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.8803 - loss: 0.2785 - val_accuracy: 0.8665 - val_loss: 0.3196\n",
      "Epoch 29/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.8836 - loss: 0.2756 - val_accuracy: 0.8565 - val_loss: 0.3404\n",
      "Epoch 30/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.8792 - loss: 0.2811 - val_accuracy: 0.8671 - val_loss: 0.3195\n",
      "Epoch 31/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.8804 - loss: 0.2828 - val_accuracy: 0.8582 - val_loss: 0.3307\n",
      "Epoch 32/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.8714 - loss: 0.2952 - val_accuracy: 0.8684 - val_loss: 0.3157\n",
      "Epoch 33/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.8777 - loss: 0.2808 - val_accuracy: 0.8677 - val_loss: 0.3224\n",
      "Epoch 34/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - accuracy: 0.8891 - loss: 0.2666 - val_accuracy: 0.8670 - val_loss: 0.3123\n",
      "Epoch 35/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.8863 - loss: 0.2654 - val_accuracy: 0.8655 - val_loss: 0.3212\n",
      "Epoch 36/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.8841 - loss: 0.2671 - val_accuracy: 0.8704 - val_loss: 0.3041\n",
      "Epoch 37/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.8906 - loss: 0.2609 - val_accuracy: 0.8536 - val_loss: 0.3397\n",
      "Epoch 38/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.8810 - loss: 0.2693 - val_accuracy: 0.8690 - val_loss: 0.3133\n",
      "Epoch 39/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.8880 - loss: 0.2612 - val_accuracy: 0.8537 - val_loss: 0.3392\n",
      "Epoch 40/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.8843 - loss: 0.2634 - val_accuracy: 0.8641 - val_loss: 0.3090\n",
      "Epoch 41/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - accuracy: 0.8871 - loss: 0.2500 - val_accuracy: 0.8593 - val_loss: 0.3268\n",
      "Epoch 42/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.8882 - loss: 0.2520 - val_accuracy: 0.8709 - val_loss: 0.3082\n",
      "Epoch 43/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.8909 - loss: 0.2507 - val_accuracy: 0.8662 - val_loss: 0.3189\n",
      "Epoch 44/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.8845 - loss: 0.2661 - val_accuracy: 0.8493 - val_loss: 0.3614\n",
      "Epoch 45/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.8769 - loss: 0.2943 - val_accuracy: 0.8688 - val_loss: 0.3198\n",
      "Epoch 46/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.8868 - loss: 0.2568 - val_accuracy: 0.8687 - val_loss: 0.2936\n",
      "Epoch 47/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.8869 - loss: 0.2514 - val_accuracy: 0.8634 - val_loss: 0.3092\n",
      "Epoch 48/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - accuracy: 0.8897 - loss: 0.2525 - val_accuracy: 0.8748 - val_loss: 0.2966\n",
      "Epoch 49/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.8933 - loss: 0.2405 - val_accuracy: 0.8747 - val_loss: 0.3006\n",
      "Epoch 50/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.8946 - loss: 0.2396 - val_accuracy: 0.8727 - val_loss: 0.2968\n",
      "Epoch 51/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.8878 - loss: 0.2458 - val_accuracy: 0.8731 - val_loss: 0.2989\n",
      "Epoch 52/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.8922 - loss: 0.2382 - val_accuracy: 0.8740 - val_loss: 0.2941\n",
      "Epoch 53/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.8900 - loss: 0.2375 - val_accuracy: 0.8726 - val_loss: 0.2915\n",
      "Epoch 54/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.8886 - loss: 0.2443 - val_accuracy: 0.8652 - val_loss: 0.3127\n",
      "Epoch 55/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.8898 - loss: 0.2485 - val_accuracy: 0.8702 - val_loss: 0.2928\n",
      "Epoch 56/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.8922 - loss: 0.2399 - val_accuracy: 0.8718 - val_loss: 0.2960\n",
      "Epoch 57/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.8886 - loss: 0.2423 - val_accuracy: 0.8700 - val_loss: 0.3030\n",
      "Epoch 58/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.8909 - loss: 0.2389 - val_accuracy: 0.8727 - val_loss: 0.3059\n",
      "Epoch 59/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.8907 - loss: 0.2348 - val_accuracy: 0.8730 - val_loss: 0.3021\n",
      "Epoch 60/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.8897 - loss: 0.2400 - val_accuracy: 0.8715 - val_loss: 0.3152\n",
      "\n",
      "Finished training CNN model 1/9\n",
      "    n_total = 1.0e+05 samples\n",
      "         lr = 0.05\n",
      "Best test error rate was 12.5200% in epoch 48/60\n",
      "\n",
      "Training CNN model 2/9\n",
      "    n_total = 1.0e+05 samples\n",
      "         lr = 0.005\n",
      "----------------------------\n",
      "Number of unique contributions: 13\n",
      "Total number of fractions: 28\n",
      "Total number of phases: 62\n",
      "Epoch 1/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 200ms/step - accuracy: 0.6802 - loss: 0.5882 - val_accuracy: 0.7710 - val_loss: 0.4522\n",
      "Epoch 2/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.7775 - loss: 0.4374 - val_accuracy: 0.7722 - val_loss: 0.4061\n",
      "Epoch 3/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.7914 - loss: 0.3867 - val_accuracy: 0.8032 - val_loss: 0.3749\n",
      "Epoch 4/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.8322 - loss: 0.3423 - val_accuracy: 0.8501 - val_loss: 0.3191\n",
      "Epoch 5/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.8609 - loss: 0.3010 - val_accuracy: 0.8692 - val_loss: 0.2951\n",
      "Epoch 6/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.8761 - loss: 0.2748 - val_accuracy: 0.8772 - val_loss: 0.2811\n",
      "Epoch 7/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - accuracy: 0.8848 - loss: 0.2570 - val_accuracy: 0.8810 - val_loss: 0.2800\n",
      "Epoch 8/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.8931 - loss: 0.2428 - val_accuracy: 0.8880 - val_loss: 0.2585\n",
      "Epoch 9/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.9033 - loss: 0.2236 - val_accuracy: 0.8942 - val_loss: 0.2545\n",
      "Epoch 10/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.9164 - loss: 0.1971 - val_accuracy: 0.8954 - val_loss: 0.2480\n",
      "Epoch 11/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.9227 - loss: 0.1849 - val_accuracy: 0.8979 - val_loss: 0.2430\n",
      "Epoch 12/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.9319 - loss: 0.1687 - val_accuracy: 0.8985 - val_loss: 0.2427\n",
      "Epoch 13/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.9356 - loss: 0.1596 - val_accuracy: 0.9013 - val_loss: 0.2456\n",
      "Epoch 14/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.9426 - loss: 0.1482 - val_accuracy: 0.9039 - val_loss: 0.2442\n",
      "Epoch 15/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.9453 - loss: 0.1370 - val_accuracy: 0.9038 - val_loss: 0.2404\n",
      "Epoch 16/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.9496 - loss: 0.1290 - val_accuracy: 0.9042 - val_loss: 0.2484\n",
      "Epoch 17/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.9496 - loss: 0.1209 - val_accuracy: 0.9021 - val_loss: 0.2520\n",
      "Epoch 18/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.9547 - loss: 0.1167 - val_accuracy: 0.9050 - val_loss: 0.2512\n",
      "Epoch 19/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.9620 - loss: 0.1000 - val_accuracy: 0.9070 - val_loss: 0.2589\n",
      "Epoch 20/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.9660 - loss: 0.0895 - val_accuracy: 0.9025 - val_loss: 0.2932\n",
      "Epoch 21/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.9616 - loss: 0.0982 - val_accuracy: 0.9040 - val_loss: 0.2656\n",
      "Epoch 22/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.9710 - loss: 0.0826 - val_accuracy: 0.9084 - val_loss: 0.2735\n",
      "Epoch 23/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.9716 - loss: 0.0760 - val_accuracy: 0.9085 - val_loss: 0.2982\n",
      "Epoch 24/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 75ms/step - accuracy: 0.9747 - loss: 0.0688 - val_accuracy: 0.9068 - val_loss: 0.2945\n",
      "Epoch 25/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.9778 - loss: 0.0601 - val_accuracy: 0.9054 - val_loss: 0.3138\n",
      "Epoch 26/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.9828 - loss: 0.0528 - val_accuracy: 0.9049 - val_loss: 0.3188\n",
      "Epoch 27/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.9826 - loss: 0.0482 - val_accuracy: 0.9051 - val_loss: 0.3259\n",
      "Epoch 28/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.9882 - loss: 0.0410 - val_accuracy: 0.9074 - val_loss: 0.3437\n",
      "Epoch 29/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9865 - loss: 0.0403 - val_accuracy: 0.8967 - val_loss: 0.3614\n",
      "Epoch 30/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.9838 - loss: 0.0458 - val_accuracy: 0.9077 - val_loss: 0.3751\n",
      "Epoch 31/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - accuracy: 0.9859 - loss: 0.0404 - val_accuracy: 0.9085 - val_loss: 0.3784\n",
      "Epoch 32/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.9869 - loss: 0.0381 - val_accuracy: 0.9021 - val_loss: 0.3824\n",
      "Epoch 33/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - accuracy: 0.9846 - loss: 0.0420 - val_accuracy: 0.9045 - val_loss: 0.3778\n",
      "Epoch 34/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9884 - loss: 0.0344 - val_accuracy: 0.9068 - val_loss: 0.3889\n",
      "Epoch 35/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - accuracy: 0.9889 - loss: 0.0335 - val_accuracy: 0.9056 - val_loss: 0.3976\n",
      "Epoch 36/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 82ms/step - accuracy: 0.9891 - loss: 0.0331 - val_accuracy: 0.9042 - val_loss: 0.4063\n",
      "Epoch 37/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - accuracy: 0.9914 - loss: 0.0270 - val_accuracy: 0.9016 - val_loss: 0.4261\n",
      "Epoch 38/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.9923 - loss: 0.0241 - val_accuracy: 0.9030 - val_loss: 0.4257\n",
      "Epoch 39/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.9954 - loss: 0.0183 - val_accuracy: 0.9000 - val_loss: 0.4380\n",
      "Epoch 40/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - accuracy: 0.9955 - loss: 0.0173 - val_accuracy: 0.9043 - val_loss: 0.4410\n",
      "Epoch 41/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.9976 - loss: 0.0125 - val_accuracy: 0.9083 - val_loss: 0.4817\n",
      "Epoch 42/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - accuracy: 0.9966 - loss: 0.0135 - val_accuracy: 0.9053 - val_loss: 0.4678\n",
      "Epoch 43/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 79ms/step - accuracy: 0.9974 - loss: 0.0110 - val_accuracy: 0.9051 - val_loss: 0.4757\n",
      "Epoch 44/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - accuracy: 0.9979 - loss: 0.0105 - val_accuracy: 0.9069 - val_loss: 0.4987\n",
      "Epoch 45/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.9973 - loss: 0.0109 - val_accuracy: 0.9057 - val_loss: 0.5220\n",
      "Epoch 46/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9958 - loss: 0.0139 - val_accuracy: 0.9015 - val_loss: 0.5183\n",
      "Epoch 47/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - accuracy: 0.9935 - loss: 0.0197 - val_accuracy: 0.9006 - val_loss: 0.5239\n",
      "Epoch 48/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - accuracy: 0.9903 - loss: 0.0292 - val_accuracy: 0.9006 - val_loss: 0.5282\n",
      "Epoch 49/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.9881 - loss: 0.0360 - val_accuracy: 0.9015 - val_loss: 0.5086\n",
      "Epoch 50/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 81ms/step - accuracy: 0.9829 - loss: 0.0446 - val_accuracy: 0.8999 - val_loss: 0.4766\n",
      "Epoch 51/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.9793 - loss: 0.0510 - val_accuracy: 0.9003 - val_loss: 0.4885\n",
      "Epoch 52/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.9828 - loss: 0.0450 - val_accuracy: 0.9035 - val_loss: 0.4803\n",
      "Epoch 53/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.9864 - loss: 0.0370 - val_accuracy: 0.9051 - val_loss: 0.4604\n",
      "Epoch 54/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.9900 - loss: 0.0249 - val_accuracy: 0.9070 - val_loss: 0.4594\n",
      "Epoch 55/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.9940 - loss: 0.0192 - val_accuracy: 0.9083 - val_loss: 0.5023\n",
      "Epoch 56/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.9961 - loss: 0.0146 - val_accuracy: 0.9101 - val_loss: 0.4895\n",
      "Epoch 57/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.9972 - loss: 0.0103 - val_accuracy: 0.9095 - val_loss: 0.5137\n",
      "Epoch 58/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.9985 - loss: 0.0062 - val_accuracy: 0.9105 - val_loss: 0.5069\n",
      "Epoch 59/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.9997 - loss: 0.0037 - val_accuracy: 0.9098 - val_loss: 0.5279\n",
      "Epoch 60/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.9998 - loss: 0.0027 - val_accuracy: 0.9103 - val_loss: 0.5348\n",
      "\n",
      "Finished training CNN model 2/9\n",
      "    n_total = 1.0e+05 samples\n",
      "         lr = 0.005\n",
      "Best test error rate was 8.9525% in epoch 58/60\n",
      "\n",
      "Training CNN model 3/9\n",
      "    n_total = 1.0e+05 samples\n",
      "         lr = 0.005\n",
      "----------------------------\n",
      "Number of unique contributions: 13\n",
      "Total number of fractions: 28\n",
      "Total number of phases: 62\n",
      "Epoch 1/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 189ms/step - accuracy: 0.6912 - loss: 0.5881 - val_accuracy: 0.7742 - val_loss: 0.4582\n",
      "Epoch 2/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.7839 - loss: 0.4356 - val_accuracy: 0.7838 - val_loss: 0.4103\n",
      "Epoch 3/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.7999 - loss: 0.3896 - val_accuracy: 0.8266 - val_loss: 0.3560\n",
      "Epoch 4/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.8375 - loss: 0.3377 - val_accuracy: 0.8476 - val_loss: 0.3244\n",
      "Epoch 5/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - accuracy: 0.8680 - loss: 0.2932 - val_accuracy: 0.8688 - val_loss: 0.2923\n",
      "Epoch 6/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - accuracy: 0.8794 - loss: 0.2710 - val_accuracy: 0.8827 - val_loss: 0.2735\n",
      "Epoch 7/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.8970 - loss: 0.2378 - val_accuracy: 0.8890 - val_loss: 0.2572\n",
      "Epoch 8/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.9099 - loss: 0.2150 - val_accuracy: 0.8882 - val_loss: 0.2541\n",
      "Epoch 9/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - accuracy: 0.9175 - loss: 0.2018 - val_accuracy: 0.8982 - val_loss: 0.2379\n",
      "Epoch 10/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.9244 - loss: 0.1811 - val_accuracy: 0.9033 - val_loss: 0.2275\n",
      "Epoch 11/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - accuracy: 0.9350 - loss: 0.1602 - val_accuracy: 0.9042 - val_loss: 0.2288\n",
      "Epoch 12/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - accuracy: 0.9383 - loss: 0.1522 - val_accuracy: 0.9043 - val_loss: 0.2452\n",
      "Epoch 13/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.9400 - loss: 0.1458 - val_accuracy: 0.9040 - val_loss: 0.2316\n",
      "Epoch 14/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.9445 - loss: 0.1363 - val_accuracy: 0.9069 - val_loss: 0.2369\n",
      "Epoch 15/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.9539 - loss: 0.1186 - val_accuracy: 0.9047 - val_loss: 0.2377\n",
      "Epoch 16/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.9549 - loss: 0.1163 - val_accuracy: 0.9069 - val_loss: 0.2610\n",
      "Epoch 17/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - accuracy: 0.9572 - loss: 0.1090 - val_accuracy: 0.9052 - val_loss: 0.2558\n",
      "Epoch 18/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.9578 - loss: 0.1045 - val_accuracy: 0.9006 - val_loss: 0.2687\n",
      "Epoch 19/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.9632 - loss: 0.0980 - val_accuracy: 0.9075 - val_loss: 0.2638\n",
      "Epoch 20/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.9689 - loss: 0.0807 - val_accuracy: 0.9044 - val_loss: 0.2802\n",
      "Epoch 21/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9684 - loss: 0.0841 - val_accuracy: 0.9069 - val_loss: 0.2797\n",
      "Epoch 22/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.9742 - loss: 0.0714 - val_accuracy: 0.9034 - val_loss: 0.2981\n",
      "Epoch 23/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.9725 - loss: 0.0731 - val_accuracy: 0.9043 - val_loss: 0.3057\n",
      "Epoch 24/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.9772 - loss: 0.0630 - val_accuracy: 0.9069 - val_loss: 0.3240\n",
      "Epoch 25/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9785 - loss: 0.0623 - val_accuracy: 0.9007 - val_loss: 0.3243\n",
      "Epoch 26/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.9798 - loss: 0.0577 - val_accuracy: 0.9048 - val_loss: 0.3338\n",
      "Epoch 27/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - accuracy: 0.9818 - loss: 0.0516 - val_accuracy: 0.9012 - val_loss: 0.3442\n",
      "Epoch 28/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.9833 - loss: 0.0494 - val_accuracy: 0.9047 - val_loss: 0.3600\n",
      "Epoch 29/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.9868 - loss: 0.0414 - val_accuracy: 0.8974 - val_loss: 0.3757\n",
      "Epoch 30/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 0.9871 - loss: 0.0411 - val_accuracy: 0.9049 - val_loss: 0.3648\n",
      "Epoch 31/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.9873 - loss: 0.0387 - val_accuracy: 0.9028 - val_loss: 0.3860\n",
      "Epoch 32/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - accuracy: 0.9904 - loss: 0.0325 - val_accuracy: 0.9014 - val_loss: 0.4167\n",
      "Epoch 33/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.9887 - loss: 0.0360 - val_accuracy: 0.8981 - val_loss: 0.4098\n",
      "Epoch 34/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.9880 - loss: 0.0349 - val_accuracy: 0.9027 - val_loss: 0.4166\n",
      "Epoch 35/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - accuracy: 0.9884 - loss: 0.0337 - val_accuracy: 0.9000 - val_loss: 0.4258\n",
      "Epoch 36/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 125ms/step - accuracy: 0.9913 - loss: 0.0295 - val_accuracy: 0.9002 - val_loss: 0.4209\n",
      "Epoch 37/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.9926 - loss: 0.0240 - val_accuracy: 0.9002 - val_loss: 0.4403\n",
      "Epoch 38/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - accuracy: 0.9926 - loss: 0.0236 - val_accuracy: 0.9002 - val_loss: 0.4478\n",
      "Epoch 39/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 81ms/step - accuracy: 0.9941 - loss: 0.0196 - val_accuracy: 0.9031 - val_loss: 0.4609\n",
      "Epoch 40/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - accuracy: 0.9954 - loss: 0.0170 - val_accuracy: 0.9031 - val_loss: 0.4799\n",
      "Epoch 41/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - accuracy: 0.9965 - loss: 0.0153 - val_accuracy: 0.8973 - val_loss: 0.4894\n",
      "Epoch 42/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9944 - loss: 0.0176 - val_accuracy: 0.8964 - val_loss: 0.4899\n",
      "Epoch 43/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - accuracy: 0.9926 - loss: 0.0235 - val_accuracy: 0.9016 - val_loss: 0.5010\n",
      "Epoch 44/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.9922 - loss: 0.0215 - val_accuracy: 0.9007 - val_loss: 0.4865\n",
      "Epoch 45/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.9925 - loss: 0.0212 - val_accuracy: 0.8988 - val_loss: 0.5163\n",
      "Epoch 46/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.9912 - loss: 0.0245 - val_accuracy: 0.9002 - val_loss: 0.5190\n",
      "Epoch 47/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - accuracy: 0.9933 - loss: 0.0193 - val_accuracy: 0.9031 - val_loss: 0.5227\n",
      "Epoch 48/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - accuracy: 0.9904 - loss: 0.0251 - val_accuracy: 0.8990 - val_loss: 0.5217\n",
      "Epoch 49/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.9862 - loss: 0.0372 - val_accuracy: 0.9019 - val_loss: 0.5438\n",
      "Epoch 50/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 82ms/step - accuracy: 0.9848 - loss: 0.0416 - val_accuracy: 0.8960 - val_loss: 0.5312\n",
      "Epoch 51/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - accuracy: 0.9852 - loss: 0.0393 - val_accuracy: 0.9039 - val_loss: 0.4969\n",
      "Epoch 52/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - accuracy: 0.9887 - loss: 0.0304 - val_accuracy: 0.9026 - val_loss: 0.4931\n",
      "Epoch 53/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.9926 - loss: 0.0218 - val_accuracy: 0.9027 - val_loss: 0.4995\n",
      "Epoch 54/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.9949 - loss: 0.0163 - val_accuracy: 0.9048 - val_loss: 0.5063\n",
      "Epoch 55/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.9972 - loss: 0.0122 - val_accuracy: 0.9056 - val_loss: 0.5164\n",
      "Epoch 56/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.9989 - loss: 0.0065 - val_accuracy: 0.9040 - val_loss: 0.5361\n",
      "Epoch 57/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.9996 - loss: 0.0040 - val_accuracy: 0.9042 - val_loss: 0.5476\n",
      "Epoch 58/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 122ms/step - accuracy: 1.0000 - loss: 0.0022 - val_accuracy: 0.9069 - val_loss: 0.5520\n",
      "Epoch 59/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.9071 - val_loss: 0.5606\n",
      "Epoch 60/60\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 1.0000 - loss: 9.3142e-04 - val_accuracy: 0.9075 - val_loss: 0.5729\n",
      "\n",
      "Finished training CNN model 3/9\n",
      "    n_total = 1.0e+05 samples\n",
      "         lr = 0.005\n",
      "Best test error rate was 9.2450% in epoch 19/60\n",
      "\n",
      "Training CNN model 4/9\n",
      "    n_total = 1.0e+06 samples\n",
      "         lr = 0.05\n",
      "----------------------------\n",
      "Number of unique contributions: 13\n",
      "Total number of fractions: 28\n",
      "Total number of phases: 62\n",
      "Epoch 1/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 90ms/step - accuracy: 0.7595 - loss: 0.8318 - val_accuracy: 0.8329 - val_loss: 0.3218\n",
      "Epoch 2/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 63ms/step - accuracy: 0.8438 - loss: 0.3143 - val_accuracy: 0.8617 - val_loss: 0.2990\n",
      "Epoch 3/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 66ms/step - accuracy: 0.8789 - loss: 0.2720 - val_accuracy: 0.8931 - val_loss: 0.2521\n",
      "Epoch 4/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 62ms/step - accuracy: 0.8943 - loss: 0.2498 - val_accuracy: 0.8944 - val_loss: 0.2552\n",
      "Epoch 5/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 57ms/step - accuracy: 0.8965 - loss: 0.2440 - val_accuracy: 0.8976 - val_loss: 0.2414\n",
      "Epoch 6/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - accuracy: 0.8977 - loss: 0.2416 - val_accuracy: 0.9015 - val_loss: 0.2349\n",
      "Epoch 7/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 64ms/step - accuracy: 0.9015 - loss: 0.2358 - val_accuracy: 0.9037 - val_loss: 0.2264\n",
      "Epoch 8/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 59ms/step - accuracy: 0.9041 - loss: 0.2283 - val_accuracy: 0.8994 - val_loss: 0.2424\n",
      "Epoch 9/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 60ms/step - accuracy: 0.9078 - loss: 0.2246 - val_accuracy: 0.9123 - val_loss: 0.2119\n",
      "Epoch 10/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - accuracy: 0.9131 - loss: 0.2120 - val_accuracy: 0.9090 - val_loss: 0.2134\n",
      "Epoch 11/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 64ms/step - accuracy: 0.9100 - loss: 0.2181 - val_accuracy: 0.9089 - val_loss: 0.2263\n",
      "Epoch 12/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 64ms/step - accuracy: 0.9106 - loss: 0.2183 - val_accuracy: 0.9160 - val_loss: 0.2051\n",
      "Epoch 13/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 59ms/step - accuracy: 0.9137 - loss: 0.2099 - val_accuracy: 0.9164 - val_loss: 0.2062\n",
      "Epoch 14/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - accuracy: 0.9134 - loss: 0.2112 - val_accuracy: 0.9141 - val_loss: 0.2064\n",
      "Epoch 15/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - accuracy: 0.9146 - loss: 0.2095 - val_accuracy: 0.9182 - val_loss: 0.1974\n",
      "Epoch 16/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 63ms/step - accuracy: 0.9149 - loss: 0.2060 - val_accuracy: 0.9135 - val_loss: 0.2110\n",
      "Epoch 17/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 63ms/step - accuracy: 0.9126 - loss: 0.2052 - val_accuracy: 0.9108 - val_loss: 0.2167\n",
      "Epoch 18/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 60ms/step - accuracy: 0.9162 - loss: 0.2025 - val_accuracy: 0.9132 - val_loss: 0.2165\n",
      "Epoch 19/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 60ms/step - accuracy: 0.9176 - loss: 0.2004 - val_accuracy: 0.9192 - val_loss: 0.1970\n",
      "Epoch 20/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - accuracy: 0.9154 - loss: 0.2023 - val_accuracy: 0.9162 - val_loss: 0.2017\n",
      "Epoch 21/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - accuracy: 0.9163 - loss: 0.2020 - val_accuracy: 0.9136 - val_loss: 0.2083\n",
      "Epoch 22/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 59ms/step - accuracy: 0.9158 - loss: 0.2031 - val_accuracy: 0.9158 - val_loss: 0.2013\n",
      "Epoch 23/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 60ms/step - accuracy: 0.9146 - loss: 0.2017 - val_accuracy: 0.9189 - val_loss: 0.1945\n",
      "Epoch 24/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - accuracy: 0.9149 - loss: 0.2051 - val_accuracy: 0.9121 - val_loss: 0.2139\n",
      "Epoch 25/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 60ms/step - accuracy: 0.9125 - loss: 0.2087 - val_accuracy: 0.9143 - val_loss: 0.2081\n",
      "Epoch 26/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 62ms/step - accuracy: 0.9144 - loss: 0.2065 - val_accuracy: 0.9131 - val_loss: 0.2172\n",
      "Epoch 27/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 60ms/step - accuracy: 0.9159 - loss: 0.2064 - val_accuracy: 0.9175 - val_loss: 0.2006\n",
      "Epoch 28/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - accuracy: 0.9183 - loss: 0.1980 - val_accuracy: 0.9109 - val_loss: 0.2208\n",
      "Epoch 29/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 59ms/step - accuracy: 0.9170 - loss: 0.2063 - val_accuracy: 0.9206 - val_loss: 0.1944\n",
      "Epoch 30/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 62ms/step - accuracy: 0.9224 - loss: 0.1916 - val_accuracy: 0.9234 - val_loss: 0.2014\n",
      "Epoch 31/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 63ms/step - accuracy: 0.9234 - loss: 0.1910 - val_accuracy: 0.9255 - val_loss: 0.1830\n",
      "Epoch 32/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 63ms/step - accuracy: 0.9260 - loss: 0.1831 - val_accuracy: 0.9268 - val_loss: 0.1776\n",
      "Epoch 33/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 62ms/step - accuracy: 0.9260 - loss: 0.1820 - val_accuracy: 0.9249 - val_loss: 0.1843\n",
      "Epoch 34/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 65ms/step - accuracy: 0.9255 - loss: 0.1847 - val_accuracy: 0.9151 - val_loss: 0.2108\n",
      "Epoch 35/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 66ms/step - accuracy: 0.9264 - loss: 0.1818 - val_accuracy: 0.9256 - val_loss: 0.1849\n",
      "Epoch 36/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 65ms/step - accuracy: 0.9241 - loss: 0.1863 - val_accuracy: 0.9279 - val_loss: 0.1797\n",
      "Epoch 37/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 63ms/step - accuracy: 0.9278 - loss: 0.1788 - val_accuracy: 0.9245 - val_loss: 0.1884\n",
      "Epoch 38/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - accuracy: 0.9235 - loss: 0.1915 - val_accuracy: 0.9261 - val_loss: 0.1810\n",
      "Epoch 39/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - accuracy: 0.9266 - loss: 0.1851 - val_accuracy: 0.9256 - val_loss: 0.1842\n",
      "Epoch 40/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 59ms/step - accuracy: 0.9281 - loss: 0.1767 - val_accuracy: 0.9247 - val_loss: 0.1811\n",
      "Epoch 41/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 59ms/step - accuracy: 0.9259 - loss: 0.1814 - val_accuracy: 0.9257 - val_loss: 0.1812\n",
      "Epoch 42/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 57ms/step - accuracy: 0.9262 - loss: 0.1817 - val_accuracy: 0.9239 - val_loss: 0.1842\n",
      "Epoch 43/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 60ms/step - accuracy: 0.9241 - loss: 0.1837 - val_accuracy: 0.9261 - val_loss: 0.1833\n",
      "Epoch 44/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 59ms/step - accuracy: 0.9211 - loss: 0.1880 - val_accuracy: 0.9254 - val_loss: 0.1857\n",
      "Epoch 45/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - accuracy: 0.9237 - loss: 0.1848 - val_accuracy: 0.9250 - val_loss: 0.1820\n",
      "Epoch 46/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 62ms/step - accuracy: 0.9249 - loss: 0.1837 - val_accuracy: 0.9245 - val_loss: 0.1866\n",
      "Epoch 47/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 64ms/step - accuracy: 0.9239 - loss: 0.1885 - val_accuracy: 0.9236 - val_loss: 0.1865\n",
      "Epoch 48/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 68ms/step - accuracy: 0.9241 - loss: 0.1851 - val_accuracy: 0.9239 - val_loss: 0.1869\n",
      "Epoch 49/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 63ms/step - accuracy: 0.9257 - loss: 0.1824 - val_accuracy: 0.9247 - val_loss: 0.1855\n",
      "Epoch 50/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - accuracy: 0.9256 - loss: 0.1821 - val_accuracy: 0.9256 - val_loss: 0.1805\n",
      "Epoch 51/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - accuracy: 0.9241 - loss: 0.1852 - val_accuracy: 0.9260 - val_loss: 0.1833\n",
      "Epoch 52/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 59ms/step - accuracy: 0.9269 - loss: 0.1815 - val_accuracy: 0.9216 - val_loss: 0.1929\n",
      "Epoch 53/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - accuracy: 0.9226 - loss: 0.1891 - val_accuracy: 0.9266 - val_loss: 0.1807\n",
      "Epoch 54/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 59ms/step - accuracy: 0.9258 - loss: 0.1837 - val_accuracy: 0.9255 - val_loss: 0.1826\n",
      "Epoch 55/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 59ms/step - accuracy: 0.9270 - loss: 0.1810 - val_accuracy: 0.9256 - val_loss: 0.1844\n",
      "Epoch 56/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 59ms/step - accuracy: 0.9263 - loss: 0.1834 - val_accuracy: 0.9182 - val_loss: 0.1935\n",
      "Epoch 57/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 60ms/step - accuracy: 0.9247 - loss: 0.1857 - val_accuracy: 0.9118 - val_loss: 0.2280\n",
      "Epoch 58/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 64ms/step - accuracy: 0.9177 - loss: 0.2054 - val_accuracy: 0.9229 - val_loss: 0.1899\n",
      "Epoch 59/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 62ms/step - accuracy: 0.9265 - loss: 0.1844 - val_accuracy: 0.9268 - val_loss: 0.1821\n",
      "Epoch 60/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 62ms/step - accuracy: 0.9267 - loss: 0.1813 - val_accuracy: 0.9248 - val_loss: 0.1842\n",
      "\n",
      "Finished training CNN model 4/9\n",
      "    n_total = 1.0e+06 samples\n",
      "         lr = 0.05\n",
      "Best test error rate was 7.2137% in epoch 36/60\n",
      "\n",
      "Training CNN model 5/9\n",
      "    n_total = 1.0e+06 samples\n",
      "         lr = 0.005\n",
      "----------------------------\n",
      "Number of unique contributions: 13\n",
      "Total number of fractions: 28\n",
      "Total number of phases: 62\n",
      "Epoch 1/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 74ms/step - accuracy: 0.7890 - loss: 0.4248 - val_accuracy: 0.9055 - val_loss: 0.2216\n",
      "Epoch 2/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 60ms/step - accuracy: 0.9126 - loss: 0.2037 - val_accuracy: 0.9284 - val_loss: 0.1681\n",
      "Epoch 3/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 62ms/step - accuracy: 0.9335 - loss: 0.1597 - val_accuracy: 0.9383 - val_loss: 0.1555\n",
      "Epoch 4/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 62ms/step - accuracy: 0.9426 - loss: 0.1413 - val_accuracy: 0.9418 - val_loss: 0.1447\n",
      "Epoch 5/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 60ms/step - accuracy: 0.9477 - loss: 0.1309 - val_accuracy: 0.9453 - val_loss: 0.1362\n",
      "Epoch 6/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 62ms/step - accuracy: 0.9502 - loss: 0.1255 - val_accuracy: 0.9482 - val_loss: 0.1295\n",
      "Epoch 7/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 60ms/step - accuracy: 0.9516 - loss: 0.1213 - val_accuracy: 0.9494 - val_loss: 0.1280\n",
      "Epoch 8/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 63ms/step - accuracy: 0.9539 - loss: 0.1172 - val_accuracy: 0.9496 - val_loss: 0.1264\n",
      "Epoch 9/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 63ms/step - accuracy: 0.9551 - loss: 0.1136 - val_accuracy: 0.9509 - val_loss: 0.1234\n",
      "Epoch 10/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - accuracy: 0.9551 - loss: 0.1127 - val_accuracy: 0.9516 - val_loss: 0.1228\n",
      "Epoch 11/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 64ms/step - accuracy: 0.9558 - loss: 0.1125 - val_accuracy: 0.9500 - val_loss: 0.1259\n",
      "Epoch 12/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 63ms/step - accuracy: 0.9561 - loss: 0.1115 - val_accuracy: 0.9517 - val_loss: 0.1224\n",
      "Epoch 13/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 59ms/step - accuracy: 0.9577 - loss: 0.1085 - val_accuracy: 0.9515 - val_loss: 0.1243\n",
      "Epoch 14/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 60ms/step - accuracy: 0.9586 - loss: 0.1062 - val_accuracy: 0.9526 - val_loss: 0.1222\n",
      "Epoch 15/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - accuracy: 0.9593 - loss: 0.1039 - val_accuracy: 0.9523 - val_loss: 0.1217\n",
      "Epoch 16/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - accuracy: 0.9601 - loss: 0.1035 - val_accuracy: 0.9518 - val_loss: 0.1232\n",
      "Epoch 17/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - accuracy: 0.9606 - loss: 0.1017 - val_accuracy: 0.9535 - val_loss: 0.1224\n",
      "Epoch 18/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - accuracy: 0.9604 - loss: 0.1016 - val_accuracy: 0.9513 - val_loss: 0.1247\n",
      "Epoch 19/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 59ms/step - accuracy: 0.9615 - loss: 0.0992 - val_accuracy: 0.9533 - val_loss: 0.1213\n",
      "Epoch 20/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 62ms/step - accuracy: 0.9604 - loss: 0.1012 - val_accuracy: 0.9543 - val_loss: 0.1198\n",
      "Epoch 21/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 63ms/step - accuracy: 0.9622 - loss: 0.0977 - val_accuracy: 0.9528 - val_loss: 0.1212\n",
      "Epoch 22/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 65ms/step - accuracy: 0.9628 - loss: 0.0965 - val_accuracy: 0.9529 - val_loss: 0.1219\n",
      "Epoch 23/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 60ms/step - accuracy: 0.9625 - loss: 0.0964 - val_accuracy: 0.9513 - val_loss: 0.1280\n",
      "Epoch 24/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - accuracy: 0.9631 - loss: 0.0956 - val_accuracy: 0.9543 - val_loss: 0.1208\n",
      "Epoch 25/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 59ms/step - accuracy: 0.9645 - loss: 0.0922 - val_accuracy: 0.9530 - val_loss: 0.1258\n",
      "Epoch 26/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 59ms/step - accuracy: 0.9644 - loss: 0.0917 - val_accuracy: 0.9535 - val_loss: 0.1227\n",
      "Epoch 27/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 59ms/step - accuracy: 0.9654 - loss: 0.0905 - val_accuracy: 0.9533 - val_loss: 0.1250\n",
      "Epoch 28/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 60ms/step - accuracy: 0.9658 - loss: 0.0890 - val_accuracy: 0.9528 - val_loss: 0.1264\n",
      "Epoch 29/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 59ms/step - accuracy: 0.9655 - loss: 0.0885 - val_accuracy: 0.9525 - val_loss: 0.1269\n",
      "Epoch 30/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - accuracy: 0.9668 - loss: 0.0869 - val_accuracy: 0.9534 - val_loss: 0.1250\n",
      "Epoch 31/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 65ms/step - accuracy: 0.9675 - loss: 0.0853 - val_accuracy: 0.9517 - val_loss: 0.1290\n",
      "Epoch 32/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 64ms/step - accuracy: 0.9668 - loss: 0.0861 - val_accuracy: 0.9526 - val_loss: 0.1295\n",
      "Epoch 33/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 62ms/step - accuracy: 0.9675 - loss: 0.0841 - val_accuracy: 0.9533 - val_loss: 0.1261\n",
      "Epoch 34/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 59ms/step - accuracy: 0.9675 - loss: 0.0846 - val_accuracy: 0.9539 - val_loss: 0.1247\n",
      "Epoch 35/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - accuracy: 0.9685 - loss: 0.0829 - val_accuracy: 0.9526 - val_loss: 0.1322\n",
      "Epoch 36/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 59ms/step - accuracy: 0.9686 - loss: 0.0824 - val_accuracy: 0.9524 - val_loss: 0.1305\n",
      "Epoch 37/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 67ms/step - accuracy: 0.9679 - loss: 0.0835 - val_accuracy: 0.9508 - val_loss: 0.1361\n",
      "Epoch 38/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 65ms/step - accuracy: 0.9693 - loss: 0.0812 - val_accuracy: 0.9518 - val_loss: 0.1356\n",
      "Epoch 39/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 60ms/step - accuracy: 0.9695 - loss: 0.0796 - val_accuracy: 0.9534 - val_loss: 0.1320\n",
      "Epoch 40/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 63ms/step - accuracy: 0.9702 - loss: 0.0787 - val_accuracy: 0.9519 - val_loss: 0.1327\n",
      "Epoch 41/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 66ms/step - accuracy: 0.9704 - loss: 0.0777 - val_accuracy: 0.9519 - val_loss: 0.1351\n",
      "Epoch 42/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 67ms/step - accuracy: 0.9702 - loss: 0.0781 - val_accuracy: 0.9528 - val_loss: 0.1344\n",
      "Epoch 43/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 65ms/step - accuracy: 0.9710 - loss: 0.0755 - val_accuracy: 0.9522 - val_loss: 0.1374\n",
      "Epoch 44/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 63ms/step - accuracy: 0.9706 - loss: 0.0768 - val_accuracy: 0.9520 - val_loss: 0.1371\n",
      "Epoch 45/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - accuracy: 0.9714 - loss: 0.0745 - val_accuracy: 0.9529 - val_loss: 0.1373\n",
      "Epoch 46/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 64ms/step - accuracy: 0.9716 - loss: 0.0741 - val_accuracy: 0.9498 - val_loss: 0.1446\n",
      "Epoch 47/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 65ms/step - accuracy: 0.9719 - loss: 0.0735 - val_accuracy: 0.9504 - val_loss: 0.1421\n",
      "Epoch 48/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 66ms/step - accuracy: 0.9728 - loss: 0.0736 - val_accuracy: 0.9514 - val_loss: 0.1446\n",
      "Epoch 49/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 63ms/step - accuracy: 0.9733 - loss: 0.0713 - val_accuracy: 0.9518 - val_loss: 0.1428\n",
      "Epoch 50/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 63ms/step - accuracy: 0.9726 - loss: 0.0717 - val_accuracy: 0.9498 - val_loss: 0.1496\n",
      "Epoch 51/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - accuracy: 0.9732 - loss: 0.0705 - val_accuracy: 0.9504 - val_loss: 0.1456\n",
      "Epoch 52/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - accuracy: 0.9739 - loss: 0.0699 - val_accuracy: 0.9511 - val_loss: 0.1507\n",
      "Epoch 53/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 59ms/step - accuracy: 0.9733 - loss: 0.0698 - val_accuracy: 0.9504 - val_loss: 0.1558\n",
      "Epoch 54/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - accuracy: 0.9735 - loss: 0.0700 - val_accuracy: 0.9500 - val_loss: 0.1541\n",
      "Epoch 55/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 60ms/step - accuracy: 0.9737 - loss: 0.0692 - val_accuracy: 0.9507 - val_loss: 0.1503\n",
      "Epoch 56/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 63ms/step - accuracy: 0.9736 - loss: 0.0686 - val_accuracy: 0.9501 - val_loss: 0.1535\n",
      "Epoch 57/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 66ms/step - accuracy: 0.9756 - loss: 0.0643 - val_accuracy: 0.9499 - val_loss: 0.1538\n",
      "Epoch 58/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 64ms/step - accuracy: 0.9748 - loss: 0.0663 - val_accuracy: 0.9499 - val_loss: 0.1553\n",
      "Epoch 59/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 65ms/step - accuracy: 0.9749 - loss: 0.0659 - val_accuracy: 0.9505 - val_loss: 0.1551\n",
      "Epoch 60/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 64ms/step - accuracy: 0.9757 - loss: 0.0642 - val_accuracy: 0.9499 - val_loss: 0.1631\n",
      "\n",
      "Finished training CNN model 5/9\n",
      "    n_total = 1.0e+06 samples\n",
      "         lr = 0.005\n",
      "Best test error rate was 4.5710% in epoch 24/60\n",
      "\n",
      "Training CNN model 6/9\n",
      "    n_total = 1.0e+06 samples\n",
      "         lr = 0.005\n",
      "----------------------------\n",
      "Number of unique contributions: 13\n",
      "Total number of fractions: 28\n",
      "Total number of phases: 62\n",
      "Epoch 1/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 68ms/step - accuracy: 0.8025 - loss: 0.4027 - val_accuracy: 0.9094 - val_loss: 0.2144\n",
      "Epoch 2/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 63ms/step - accuracy: 0.9158 - loss: 0.1975 - val_accuracy: 0.9319 - val_loss: 0.1647\n",
      "Epoch 3/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 59ms/step - accuracy: 0.9363 - loss: 0.1554 - val_accuracy: 0.9418 - val_loss: 0.1454\n",
      "Epoch 4/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - accuracy: 0.9433 - loss: 0.1403 - val_accuracy: 0.9444 - val_loss: 0.1369\n",
      "Epoch 5/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 60ms/step - accuracy: 0.9478 - loss: 0.1292 - val_accuracy: 0.9470 - val_loss: 0.1336\n",
      "Epoch 6/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - accuracy: 0.9496 - loss: 0.1266 - val_accuracy: 0.9484 - val_loss: 0.1302\n",
      "Epoch 7/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - accuracy: 0.9521 - loss: 0.1218 - val_accuracy: 0.9481 - val_loss: 0.1290\n",
      "Epoch 8/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 63ms/step - accuracy: 0.9533 - loss: 0.1172 - val_accuracy: 0.9514 - val_loss: 0.1233\n",
      "Epoch 9/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 64ms/step - accuracy: 0.9555 - loss: 0.1136 - val_accuracy: 0.9504 - val_loss: 0.1255\n",
      "Epoch 10/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 63ms/step - accuracy: 0.9560 - loss: 0.1120 - val_accuracy: 0.9517 - val_loss: 0.1238\n",
      "Epoch 11/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - accuracy: 0.9574 - loss: 0.1082 - val_accuracy: 0.9519 - val_loss: 0.1219\n",
      "Epoch 12/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 62ms/step - accuracy: 0.9590 - loss: 0.1047 - val_accuracy: 0.9516 - val_loss: 0.1241\n",
      "Epoch 13/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 59ms/step - accuracy: 0.9590 - loss: 0.1047 - val_accuracy: 0.9524 - val_loss: 0.1239\n",
      "Epoch 14/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 60ms/step - accuracy: 0.9594 - loss: 0.1044 - val_accuracy: 0.9528 - val_loss: 0.1207\n",
      "Epoch 15/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 59ms/step - accuracy: 0.9601 - loss: 0.1013 - val_accuracy: 0.9526 - val_loss: 0.1241\n",
      "Epoch 16/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 62ms/step - accuracy: 0.9623 - loss: 0.0980 - val_accuracy: 0.9529 - val_loss: 0.1242\n",
      "Epoch 17/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 62ms/step - accuracy: 0.9627 - loss: 0.0970 - val_accuracy: 0.9516 - val_loss: 0.1245\n",
      "Epoch 18/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 60ms/step - accuracy: 0.9620 - loss: 0.0976 - val_accuracy: 0.9520 - val_loss: 0.1251\n",
      "Epoch 19/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 59ms/step - accuracy: 0.9633 - loss: 0.0952 - val_accuracy: 0.9515 - val_loss: 0.1258\n",
      "Epoch 20/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 59ms/step - accuracy: 0.9641 - loss: 0.0932 - val_accuracy: 0.9524 - val_loss: 0.1239\n",
      "Epoch 21/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - accuracy: 0.9645 - loss: 0.0923 - val_accuracy: 0.9527 - val_loss: 0.1250\n",
      "Epoch 22/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - accuracy: 0.9648 - loss: 0.0905 - val_accuracy: 0.9518 - val_loss: 0.1262\n",
      "Epoch 23/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - accuracy: 0.9646 - loss: 0.0900 - val_accuracy: 0.9517 - val_loss: 0.1262\n",
      "Epoch 24/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 58ms/step - accuracy: 0.9663 - loss: 0.0878 - val_accuracy: 0.9516 - val_loss: 0.1282\n",
      "Epoch 25/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 65ms/step - accuracy: 0.9675 - loss: 0.0857 - val_accuracy: 0.9528 - val_loss: 0.1263\n",
      "Epoch 26/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 63ms/step - accuracy: 0.9676 - loss: 0.0857 - val_accuracy: 0.9530 - val_loss: 0.1257\n",
      "Epoch 27/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 65ms/step - accuracy: 0.9676 - loss: 0.0840 - val_accuracy: 0.9520 - val_loss: 0.1284\n",
      "Epoch 28/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - accuracy: 0.9683 - loss: 0.0836 - val_accuracy: 0.9531 - val_loss: 0.1296\n",
      "Epoch 29/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - accuracy: 0.9678 - loss: 0.0837 - val_accuracy: 0.9501 - val_loss: 0.1345\n",
      "Epoch 30/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 63ms/step - accuracy: 0.9682 - loss: 0.0829 - val_accuracy: 0.9512 - val_loss: 0.1317\n",
      "Epoch 31/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 65ms/step - accuracy: 0.9692 - loss: 0.0812 - val_accuracy: 0.9533 - val_loss: 0.1283\n",
      "Epoch 32/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 67ms/step - accuracy: 0.9696 - loss: 0.0796 - val_accuracy: 0.9522 - val_loss: 0.1358\n",
      "Epoch 33/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 64ms/step - accuracy: 0.9696 - loss: 0.0795 - val_accuracy: 0.9517 - val_loss: 0.1379\n",
      "Epoch 34/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 63ms/step - accuracy: 0.9701 - loss: 0.0797 - val_accuracy: 0.9510 - val_loss: 0.1348\n",
      "Epoch 35/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 61ms/step - accuracy: 0.9708 - loss: 0.0772 - val_accuracy: 0.9506 - val_loss: 0.1391\n",
      "Epoch 36/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 64ms/step - accuracy: 0.9705 - loss: 0.0766 - val_accuracy: 0.9524 - val_loss: 0.1359\n",
      "Epoch 37/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 65ms/step - accuracy: 0.9732 - loss: 0.0725 - val_accuracy: 0.9512 - val_loss: 0.1407\n",
      "Epoch 38/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 62ms/step - accuracy: 0.9709 - loss: 0.0763 - val_accuracy: 0.9515 - val_loss: 0.1404\n",
      "Epoch 39/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 63ms/step - accuracy: 0.9725 - loss: 0.0723 - val_accuracy: 0.9520 - val_loss: 0.1422\n",
      "Epoch 40/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 64ms/step - accuracy: 0.9716 - loss: 0.0737 - val_accuracy: 0.9506 - val_loss: 0.1444\n",
      "Epoch 41/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 63ms/step - accuracy: 0.9735 - loss: 0.0706 - val_accuracy: 0.9496 - val_loss: 0.1477\n",
      "Epoch 42/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 67ms/step - accuracy: 0.9730 - loss: 0.0711 - val_accuracy: 0.9513 - val_loss: 0.1480\n",
      "Epoch 43/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 68ms/step - accuracy: 0.9732 - loss: 0.0702 - val_accuracy: 0.9509 - val_loss: 0.1440\n",
      "Epoch 44/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 66ms/step - accuracy: 0.9746 - loss: 0.0679 - val_accuracy: 0.9506 - val_loss: 0.1461\n",
      "Epoch 45/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 71ms/step - accuracy: 0.9742 - loss: 0.0680 - val_accuracy: 0.9517 - val_loss: 0.1471\n",
      "Epoch 46/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - accuracy: 0.9737 - loss: 0.0691 - val_accuracy: 0.9507 - val_loss: 0.1492\n",
      "Epoch 47/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - accuracy: 0.9752 - loss: 0.0666 - val_accuracy: 0.9501 - val_loss: 0.1515\n",
      "Epoch 48/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 73ms/step - accuracy: 0.9756 - loss: 0.0655 - val_accuracy: 0.9487 - val_loss: 0.1549\n",
      "Epoch 49/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - accuracy: 0.9756 - loss: 0.0643 - val_accuracy: 0.9495 - val_loss: 0.1576\n",
      "Epoch 50/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 73ms/step - accuracy: 0.9747 - loss: 0.0669 - val_accuracy: 0.9508 - val_loss: 0.1563\n",
      "Epoch 51/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - accuracy: 0.9761 - loss: 0.0630 - val_accuracy: 0.9513 - val_loss: 0.1544\n",
      "Epoch 52/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 73ms/step - accuracy: 0.9755 - loss: 0.0645 - val_accuracy: 0.9498 - val_loss: 0.1559\n",
      "Epoch 53/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - accuracy: 0.9762 - loss: 0.0630 - val_accuracy: 0.9498 - val_loss: 0.1606\n",
      "Epoch 54/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 73ms/step - accuracy: 0.9761 - loss: 0.0640 - val_accuracy: 0.9505 - val_loss: 0.1563\n",
      "Epoch 55/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - accuracy: 0.9767 - loss: 0.0621 - val_accuracy: 0.9511 - val_loss: 0.1574\n",
      "Epoch 56/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 73ms/step - accuracy: 0.9776 - loss: 0.0594 - val_accuracy: 0.9494 - val_loss: 0.1632\n",
      "Epoch 57/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - accuracy: 0.9780 - loss: 0.0597 - val_accuracy: 0.9496 - val_loss: 0.1631\n",
      "Epoch 58/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 73ms/step - accuracy: 0.9778 - loss: 0.0584 - val_accuracy: 0.9491 - val_loss: 0.1666\n",
      "Epoch 59/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 73ms/step - accuracy: 0.9775 - loss: 0.0594 - val_accuracy: 0.9505 - val_loss: 0.1645\n",
      "Epoch 60/60\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - accuracy: 0.9776 - loss: 0.0585 - val_accuracy: 0.9503 - val_loss: 0.1653\n",
      "\n",
      "Finished training CNN model 6/9\n",
      "    n_total = 1.0e+06 samples\n",
      "         lr = 0.005\n",
      "Best test error rate was 4.6722% in epoch 31/60\n",
      "\n",
      "Training CNN model 7/9\n",
      "    n_total = 1.0e+07 samples\n",
      "         lr = 0.05\n",
      "----------------------------\n",
      "Number of unique contributions: 13\n",
      "Total number of fractions: 28\n",
      "Total number of phases: 62\n",
      "Epoch 1/60\n",
      "\u001b[1m1999/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8414 - loss: 0.3835"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-22 16:17:47.623171: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1152000000 exceeds 10% of free system memory.\n",
      "2024-06-22 16:17:50.755469: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1152000000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 72ms/step - accuracy: 0.8415 - loss: 0.3834 - val_accuracy: 0.9065 - val_loss: 0.2176\n",
      "Epoch 2/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9069 - loss: 0.2202 - val_accuracy: 0.9065 - val_loss: 0.2187\n",
      "Epoch 3/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 54ms/step - accuracy: 0.9072 - loss: 0.2184 - val_accuracy: 0.9068 - val_loss: 0.2122\n",
      "Epoch 4/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 54ms/step - accuracy: 0.9126 - loss: 0.2029 - val_accuracy: 0.9163 - val_loss: 0.1985\n",
      "Epoch 5/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 54ms/step - accuracy: 0.9149 - loss: 0.1980 - val_accuracy: 0.9122 - val_loss: 0.1909\n",
      "Epoch 6/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9137 - loss: 0.1960 - val_accuracy: 0.9123 - val_loss: 0.1973\n",
      "Epoch 7/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9137 - loss: 0.1947 - val_accuracy: 0.9104 - val_loss: 0.2019\n",
      "Epoch 8/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9145 - loss: 0.1922 - val_accuracy: 0.9152 - val_loss: 0.1957\n",
      "Epoch 9/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9147 - loss: 0.1914 - val_accuracy: 0.9134 - val_loss: 0.1974\n",
      "Epoch 10/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9144 - loss: 0.1937 - val_accuracy: 0.9155 - val_loss: 0.1970\n",
      "Epoch 11/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9151 - loss: 0.1925 - val_accuracy: 0.9119 - val_loss: 0.2095\n",
      "Epoch 12/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 54ms/step - accuracy: 0.9151 - loss: 0.1919 - val_accuracy: 0.9185 - val_loss: 0.1839\n",
      "Epoch 13/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 54ms/step - accuracy: 0.9151 - loss: 0.1910 - val_accuracy: 0.9166 - val_loss: 0.1880\n",
      "Epoch 14/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9157 - loss: 0.1921 - val_accuracy: 0.9189 - val_loss: 0.1842\n",
      "Epoch 15/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9162 - loss: 0.1888 - val_accuracy: 0.9183 - val_loss: 0.1852\n",
      "Epoch 16/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9171 - loss: 0.1890 - val_accuracy: 0.9149 - val_loss: 0.1905\n",
      "Epoch 17/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9173 - loss: 0.1872 - val_accuracy: 0.9158 - val_loss: 0.1850\n",
      "Epoch 18/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9177 - loss: 0.1872 - val_accuracy: 0.9197 - val_loss: 0.1877\n",
      "Epoch 19/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9190 - loss: 0.1865 - val_accuracy: 0.9258 - val_loss: 0.1791\n",
      "Epoch 20/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9222 - loss: 0.1826 - val_accuracy: 0.9245 - val_loss: 0.1770\n",
      "Epoch 21/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9239 - loss: 0.1783 - val_accuracy: 0.9297 - val_loss: 0.1698\n",
      "Epoch 22/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9272 - loss: 0.1759 - val_accuracy: 0.9296 - val_loss: 0.1791\n",
      "Epoch 23/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9286 - loss: 0.1740 - val_accuracy: 0.9239 - val_loss: 0.1729\n",
      "Epoch 24/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9280 - loss: 0.1753 - val_accuracy: 0.9262 - val_loss: 0.1728\n",
      "Epoch 25/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 54ms/step - accuracy: 0.9284 - loss: 0.1741 - val_accuracy: 0.9336 - val_loss: 0.1648\n",
      "Epoch 26/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 54ms/step - accuracy: 0.9288 - loss: 0.1737 - val_accuracy: 0.9314 - val_loss: 0.1691\n",
      "Epoch 27/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 54ms/step - accuracy: 0.9279 - loss: 0.1761 - val_accuracy: 0.9275 - val_loss: 0.1827\n",
      "Epoch 28/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9279 - loss: 0.1820 - val_accuracy: 0.9301 - val_loss: 0.1729\n",
      "Epoch 29/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 54ms/step - accuracy: 0.9298 - loss: 0.1719 - val_accuracy: 0.9328 - val_loss: 0.1651\n",
      "Epoch 30/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9301 - loss: 0.1719 - val_accuracy: 0.9346 - val_loss: 0.1652\n",
      "Epoch 31/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 54ms/step - accuracy: 0.9304 - loss: 0.1748 - val_accuracy: 0.9331 - val_loss: 0.1693\n",
      "Epoch 32/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 54ms/step - accuracy: 0.9316 - loss: 0.1720 - val_accuracy: 0.9355 - val_loss: 0.1619\n",
      "Epoch 33/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 54ms/step - accuracy: 0.9307 - loss: 0.1715 - val_accuracy: 0.9353 - val_loss: 0.1603\n",
      "Epoch 34/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 54ms/step - accuracy: 0.9317 - loss: 0.1694 - val_accuracy: 0.9303 - val_loss: 0.1740\n",
      "Epoch 35/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 54ms/step - accuracy: 0.9305 - loss: 0.1731 - val_accuracy: 0.9257 - val_loss: 0.1851\n",
      "Epoch 36/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9302 - loss: 0.1729 - val_accuracy: 0.9302 - val_loss: 0.1765\n",
      "Epoch 37/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9300 - loss: 0.1732 - val_accuracy: 0.9282 - val_loss: 0.1774\n",
      "Epoch 38/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9299 - loss: 0.1729 - val_accuracy: 0.9296 - val_loss: 0.1758\n",
      "Epoch 39/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 54ms/step - accuracy: 0.9300 - loss: 0.1725 - val_accuracy: 0.9292 - val_loss: 0.1758\n",
      "Epoch 40/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9293 - loss: 0.1770 - val_accuracy: 0.9327 - val_loss: 0.1666\n",
      "Epoch 41/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9297 - loss: 0.1726 - val_accuracy: 0.9273 - val_loss: 0.1819\n",
      "Epoch 42/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9212 - loss: nan - val_accuracy: 0.7702 - val_loss: nan\n",
      "Epoch 43/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.7698 - loss: nan - val_accuracy: 0.7702 - val_loss: nan\n",
      "Epoch 44/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 54ms/step - accuracy: 0.7701 - loss: nan - val_accuracy: 0.7702 - val_loss: nan\n",
      "Epoch 45/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.7701 - loss: nan - val_accuracy: 0.7702 - val_loss: nan\n",
      "Epoch 46/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 54ms/step - accuracy: 0.7705 - loss: nan - val_accuracy: 0.7702 - val_loss: nan\n",
      "Epoch 47/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 54ms/step - accuracy: 0.7701 - loss: nan - val_accuracy: 0.7702 - val_loss: nan\n",
      "Epoch 48/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 54ms/step - accuracy: 0.7702 - loss: nan - val_accuracy: 0.7702 - val_loss: nan\n",
      "Epoch 49/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.7704 - loss: nan - val_accuracy: 0.7702 - val_loss: nan\n",
      "Epoch 50/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 54ms/step - accuracy: 0.7704 - loss: nan - val_accuracy: 0.7702 - val_loss: nan\n",
      "Epoch 51/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 54ms/step - accuracy: 0.7700 - loss: nan - val_accuracy: 0.7702 - val_loss: nan\n",
      "Epoch 52/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 73ms/step - accuracy: 0.7705 - loss: nan - val_accuracy: 0.7702 - val_loss: nan\n",
      "Epoch 53/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 73ms/step - accuracy: 0.7701 - loss: nan - val_accuracy: 0.7702 - val_loss: nan\n",
      "Epoch 54/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 60ms/step - accuracy: 0.7702 - loss: nan - val_accuracy: 0.7702 - val_loss: nan\n",
      "Epoch 55/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 54ms/step - accuracy: 0.7701 - loss: nan - val_accuracy: 0.7702 - val_loss: nan\n",
      "Epoch 56/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.7701 - loss: nan - val_accuracy: 0.7702 - val_loss: nan\n",
      "Epoch 57/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.7702 - loss: nan - val_accuracy: 0.7702 - val_loss: nan\n",
      "Epoch 58/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.7698 - loss: nan - val_accuracy: 0.7702 - val_loss: nan\n",
      "Epoch 59/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 54ms/step - accuracy: 0.7700 - loss: nan - val_accuracy: 0.7702 - val_loss: nan\n",
      "Epoch 60/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 54ms/step - accuracy: 0.7699 - loss: nan - val_accuracy: 0.7702 - val_loss: nan\n",
      "\n",
      "Finished training CNN model 7/9\n",
      "    n_total = 1.0e+07 samples\n",
      "         lr = 0.05\n",
      "Best test error rate was 6.4477% in epoch 32/60\n",
      "\n",
      "Training CNN model 8/9\n",
      "    n_total = 1.0e+07 samples\n",
      "         lr = 0.005\n",
      "----------------------------\n",
      "Number of unique contributions: 13\n",
      "Total number of fractions: 28\n",
      "Total number of phases: 62\n",
      "Epoch 1/60\n",
      "\u001b[1m1999/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9085 - loss: 0.2115"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-22 18:09:37.258737: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1152000000 exceeds 10% of free system memory.\n",
      "2024-06-22 18:09:40.152921: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1152000000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 58ms/step - accuracy: 0.9086 - loss: 0.2115 - val_accuracy: 0.9527 - val_loss: 0.1194\n",
      "Epoch 2/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 54ms/step - accuracy: 0.9541 - loss: 0.1169 - val_accuracy: 0.9563 - val_loss: 0.1125\n",
      "Epoch 3/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9566 - loss: 0.1114 - val_accuracy: 0.9576 - val_loss: 0.1086\n",
      "Epoch 4/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9584 - loss: 0.1072 - val_accuracy: 0.9586 - val_loss: 0.1070\n",
      "Epoch 5/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9594 - loss: 0.1051 - val_accuracy: 0.9582 - val_loss: 0.1080\n",
      "Epoch 6/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9603 - loss: 0.1031 - val_accuracy: 0.9604 - val_loss: 0.1026\n",
      "Epoch 7/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9610 - loss: 0.1018 - val_accuracy: 0.9610 - val_loss: 0.1015\n",
      "Epoch 8/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 54ms/step - accuracy: 0.9611 - loss: 0.1011 - val_accuracy: 0.9613 - val_loss: 0.1002\n",
      "Epoch 9/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 54ms/step - accuracy: 0.9616 - loss: 0.0996 - val_accuracy: 0.9616 - val_loss: 0.1000\n",
      "Epoch 10/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9618 - loss: 0.0995 - val_accuracy: 0.9611 - val_loss: 0.1010\n",
      "Epoch 11/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9621 - loss: 0.0987 - val_accuracy: 0.9608 - val_loss: 0.1027\n",
      "Epoch 12/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9625 - loss: 0.0984 - val_accuracy: 0.9614 - val_loss: 0.1013\n",
      "Epoch 13/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9628 - loss: 0.0976 - val_accuracy: 0.9628 - val_loss: 0.0978\n",
      "Epoch 14/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9626 - loss: 0.0976 - val_accuracy: 0.9615 - val_loss: 0.1019\n",
      "Epoch 15/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 54ms/step - accuracy: 0.9629 - loss: 0.0972 - val_accuracy: 0.9624 - val_loss: 0.0982\n",
      "Epoch 16/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 54ms/step - accuracy: 0.9629 - loss: 0.0968 - val_accuracy: 0.9627 - val_loss: 0.0975\n",
      "Epoch 17/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9633 - loss: 0.0964 - val_accuracy: 0.9625 - val_loss: 0.0980\n",
      "Epoch 18/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9631 - loss: 0.0965 - val_accuracy: 0.9627 - val_loss: 0.0980\n",
      "Epoch 19/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9634 - loss: 0.0961 - val_accuracy: 0.9621 - val_loss: 0.0991\n",
      "Epoch 20/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9636 - loss: 0.0959 - val_accuracy: 0.9616 - val_loss: 0.1005\n",
      "Epoch 21/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9635 - loss: 0.0959 - val_accuracy: 0.9626 - val_loss: 0.0982\n",
      "Epoch 22/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9637 - loss: 0.0951 - val_accuracy: 0.9628 - val_loss: 0.0974\n",
      "Epoch 23/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 54ms/step - accuracy: 0.9637 - loss: 0.0955 - val_accuracy: 0.9625 - val_loss: 0.0980\n",
      "Epoch 24/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 54ms/step - accuracy: 0.9636 - loss: 0.0949 - val_accuracy: 0.9629 - val_loss: 0.0972\n",
      "Epoch 25/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9637 - loss: 0.0952 - val_accuracy: 0.9623 - val_loss: 0.0991\n",
      "Epoch 26/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9635 - loss: 0.0959 - val_accuracy: 0.9623 - val_loss: 0.0989\n",
      "Epoch 27/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 54ms/step - accuracy: 0.9640 - loss: 0.0946 - val_accuracy: 0.9622 - val_loss: 0.0988\n",
      "Epoch 28/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9637 - loss: 0.0954 - val_accuracy: 0.9609 - val_loss: 0.1037\n",
      "Epoch 29/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9641 - loss: 0.0944 - val_accuracy: 0.9628 - val_loss: 0.0975\n",
      "Epoch 30/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9641 - loss: 0.0946 - val_accuracy: 0.9624 - val_loss: 0.0989\n",
      "Epoch 31/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 54ms/step - accuracy: 0.9640 - loss: 0.0946 - val_accuracy: 0.9627 - val_loss: 0.0989\n",
      "Epoch 32/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9642 - loss: 0.0944 - val_accuracy: 0.9623 - val_loss: 0.0991\n",
      "Epoch 33/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9642 - loss: 0.0945 - val_accuracy: 0.9633 - val_loss: 0.0962\n",
      "Epoch 34/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9641 - loss: 0.0944 - val_accuracy: 0.9631 - val_loss: 0.0968\n",
      "Epoch 35/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9642 - loss: 0.0943 - val_accuracy: 0.9638 - val_loss: 0.0957\n",
      "Epoch 36/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9644 - loss: 0.0939 - val_accuracy: 0.9636 - val_loss: 0.0956\n",
      "Epoch 37/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9642 - loss: 0.0937 - val_accuracy: 0.9630 - val_loss: 0.0965\n",
      "Epoch 38/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9641 - loss: 0.0944 - val_accuracy: 0.9630 - val_loss: 0.0974\n",
      "Epoch 39/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9643 - loss: 0.0942 - val_accuracy: 0.9630 - val_loss: 0.0974\n",
      "Epoch 40/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9641 - loss: 0.0943 - val_accuracy: 0.9636 - val_loss: 0.0955\n",
      "Epoch 41/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9642 - loss: 0.0940 - val_accuracy: 0.9632 - val_loss: 0.0962\n",
      "Epoch 42/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 54ms/step - accuracy: 0.9645 - loss: 0.0935 - val_accuracy: 0.9632 - val_loss: 0.0967\n",
      "Epoch 43/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9646 - loss: 0.0938 - val_accuracy: 0.9626 - val_loss: 0.0982\n",
      "Epoch 44/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9643 - loss: 0.0938 - val_accuracy: 0.9630 - val_loss: 0.0978\n",
      "Epoch 45/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9643 - loss: 0.0936 - val_accuracy: 0.9633 - val_loss: 0.0973\n",
      "Epoch 46/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9646 - loss: 0.0936 - val_accuracy: 0.9633 - val_loss: 0.0962\n",
      "Epoch 47/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9644 - loss: 0.0937 - val_accuracy: 0.9636 - val_loss: 0.0964\n",
      "Epoch 48/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9647 - loss: 0.0930 - val_accuracy: 0.9632 - val_loss: 0.0966\n",
      "Epoch 49/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9647 - loss: 0.0930 - val_accuracy: 0.9636 - val_loss: 0.0961\n",
      "Epoch 50/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9647 - loss: 0.0932 - val_accuracy: 0.9637 - val_loss: 0.0956\n",
      "Epoch 51/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9646 - loss: 0.0932 - val_accuracy: 0.9639 - val_loss: 0.0950\n",
      "Epoch 52/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9645 - loss: 0.0935 - val_accuracy: 0.9635 - val_loss: 0.0964\n",
      "Epoch 53/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9651 - loss: 0.0926 - val_accuracy: 0.9633 - val_loss: 0.0975\n",
      "Epoch 54/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9647 - loss: 0.0929 - val_accuracy: 0.9627 - val_loss: 0.0983\n",
      "Epoch 55/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9646 - loss: 0.0930 - val_accuracy: 0.9635 - val_loss: 0.0976\n",
      "Epoch 56/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9644 - loss: 0.0940 - val_accuracy: 0.9634 - val_loss: 0.0967\n",
      "Epoch 57/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9647 - loss: 0.0928 - val_accuracy: 0.9638 - val_loss: 0.0961\n",
      "Epoch 58/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9647 - loss: 0.0928 - val_accuracy: 0.9637 - val_loss: 0.0959\n",
      "Epoch 59/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9646 - loss: 0.0933 - val_accuracy: 0.9639 - val_loss: 0.0953\n",
      "Epoch 60/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 54ms/step - accuracy: 0.9649 - loss: 0.0925 - val_accuracy: 0.9623 - val_loss: 0.1017\n",
      "\n",
      "Finished training CNN model 8/9\n",
      "    n_total = 1.0e+07 samples\n",
      "         lr = 0.005\n",
      "Best test error rate was 3.6065% in epoch 51/60\n",
      "\n",
      "Training CNN model 9/9\n",
      "    n_total = 1.0e+07 samples\n",
      "         lr = 0.005\n",
      "----------------------------\n",
      "Number of unique contributions: 13\n",
      "Total number of fractions: 28\n",
      "Total number of phases: 62\n",
      "Epoch 1/60\n",
      "\u001b[1m1997/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9005 - loss: 0.2233"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-22 19:59:18.639668: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1152000000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 60ms/step - accuracy: 0.9005 - loss: 0.2232 - val_accuracy: 0.9544 - val_loss: 0.1162\n",
      "Epoch 2/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 55ms/step - accuracy: 0.9540 - loss: 0.1171 - val_accuracy: 0.9566 - val_loss: 0.1119\n",
      "Epoch 3/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 55ms/step - accuracy: 0.9573 - loss: 0.1094 - val_accuracy: 0.9581 - val_loss: 0.1089\n",
      "Epoch 4/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 55ms/step - accuracy: 0.9592 - loss: 0.1055 - val_accuracy: 0.9591 - val_loss: 0.1053\n",
      "Epoch 5/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 55ms/step - accuracy: 0.9600 - loss: 0.1036 - val_accuracy: 0.9606 - val_loss: 0.1024\n",
      "Epoch 6/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 55ms/step - accuracy: 0.9607 - loss: 0.1023 - val_accuracy: 0.9609 - val_loss: 0.1022\n",
      "Epoch 7/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 55ms/step - accuracy: 0.9614 - loss: 0.1008 - val_accuracy: 0.9613 - val_loss: 0.1015\n",
      "Epoch 8/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 55ms/step - accuracy: 0.9619 - loss: 0.0993 - val_accuracy: 0.9620 - val_loss: 0.0992\n",
      "Epoch 9/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 55ms/step - accuracy: 0.9623 - loss: 0.0986 - val_accuracy: 0.9613 - val_loss: 0.1008\n",
      "Epoch 10/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 55ms/step - accuracy: 0.9624 - loss: 0.0983 - val_accuracy: 0.9628 - val_loss: 0.0974\n",
      "Epoch 11/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 55ms/step - accuracy: 0.9626 - loss: 0.0978 - val_accuracy: 0.9628 - val_loss: 0.0981\n",
      "Epoch 12/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 55ms/step - accuracy: 0.9633 - loss: 0.0962 - val_accuracy: 0.9620 - val_loss: 0.0995\n",
      "Epoch 13/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 55ms/step - accuracy: 0.9629 - loss: 0.0971 - val_accuracy: 0.9623 - val_loss: 0.0987\n",
      "Epoch 14/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 55ms/step - accuracy: 0.9632 - loss: 0.0964 - val_accuracy: 0.9629 - val_loss: 0.0970\n",
      "Epoch 15/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 55ms/step - accuracy: 0.9633 - loss: 0.0963 - val_accuracy: 0.9626 - val_loss: 0.0992\n",
      "Epoch 16/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 55ms/step - accuracy: 0.9634 - loss: 0.0960 - val_accuracy: 0.9627 - val_loss: 0.0985\n",
      "Epoch 17/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 55ms/step - accuracy: 0.9638 - loss: 0.0950 - val_accuracy: 0.9618 - val_loss: 0.1002\n",
      "Epoch 18/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 55ms/step - accuracy: 0.9635 - loss: 0.0956 - val_accuracy: 0.9628 - val_loss: 0.0976\n",
      "Epoch 19/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 55ms/step - accuracy: 0.9637 - loss: 0.0948 - val_accuracy: 0.9630 - val_loss: 0.0970\n",
      "Epoch 20/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 55ms/step - accuracy: 0.9640 - loss: 0.0943 - val_accuracy: 0.9624 - val_loss: 0.0993\n",
      "Epoch 21/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 55ms/step - accuracy: 0.9639 - loss: 0.0944 - val_accuracy: 0.9627 - val_loss: 0.0974\n",
      "Epoch 22/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 55ms/step - accuracy: 0.9643 - loss: 0.0940 - val_accuracy: 0.9637 - val_loss: 0.0958\n",
      "Epoch 23/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 55ms/step - accuracy: 0.9643 - loss: 0.0940 - val_accuracy: 0.9628 - val_loss: 0.0980\n",
      "Epoch 24/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 55ms/step - accuracy: 0.9641 - loss: 0.0943 - val_accuracy: 0.9616 - val_loss: 0.1023\n",
      "Epoch 25/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 55ms/step - accuracy: 0.9643 - loss: 0.0941 - val_accuracy: 0.9635 - val_loss: 0.0959\n",
      "Epoch 26/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 55ms/step - accuracy: 0.9643 - loss: 0.0934 - val_accuracy: 0.9635 - val_loss: 0.0964\n",
      "Epoch 27/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 55ms/step - accuracy: 0.9646 - loss: 0.0929 - val_accuracy: 0.9640 - val_loss: 0.0956\n",
      "Epoch 28/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 55ms/step - accuracy: 0.9647 - loss: 0.0931 - val_accuracy: 0.9631 - val_loss: 0.0966\n",
      "Epoch 29/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 70ms/step - accuracy: 0.9644 - loss: 0.0935 - val_accuracy: 0.9635 - val_loss: 0.0964\n",
      "Epoch 30/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 89ms/step - accuracy: 0.9646 - loss: 0.0934 - val_accuracy: 0.9628 - val_loss: 0.0973\n",
      "Epoch 31/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 83ms/step - accuracy: 0.9646 - loss: 0.0931 - val_accuracy: 0.9635 - val_loss: 0.0965\n",
      "Epoch 32/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 84ms/step - accuracy: 0.9648 - loss: 0.0926 - val_accuracy: 0.9635 - val_loss: 0.0967\n",
      "Epoch 33/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 56ms/step - accuracy: 0.9650 - loss: 0.0924 - val_accuracy: 0.9624 - val_loss: 0.0982\n",
      "Epoch 34/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 56ms/step - accuracy: 0.9644 - loss: 0.0931 - val_accuracy: 0.9642 - val_loss: 0.0944\n",
      "Epoch 35/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 59ms/step - accuracy: 0.9649 - loss: 0.0922 - val_accuracy: 0.9627 - val_loss: 0.0975\n",
      "Epoch 36/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 61ms/step - accuracy: 0.9649 - loss: 0.0923 - val_accuracy: 0.9636 - val_loss: 0.0964\n",
      "Epoch 37/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 60ms/step - accuracy: 0.9647 - loss: 0.0926 - val_accuracy: 0.9637 - val_loss: 0.0958\n",
      "Epoch 38/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 58ms/step - accuracy: 0.9650 - loss: 0.0925 - val_accuracy: 0.9638 - val_loss: 0.0957\n",
      "Epoch 39/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 58ms/step - accuracy: 0.9650 - loss: 0.0925 - val_accuracy: 0.9633 - val_loss: 0.0969\n",
      "Epoch 40/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 59ms/step - accuracy: 0.9650 - loss: 0.0922 - val_accuracy: 0.9641 - val_loss: 0.0946\n",
      "Epoch 41/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 59ms/step - accuracy: 0.9650 - loss: 0.0922 - val_accuracy: 0.9638 - val_loss: 0.0953\n",
      "Epoch 42/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 58ms/step - accuracy: 0.9647 - loss: 0.0930 - val_accuracy: 0.9642 - val_loss: 0.0953\n",
      "Epoch 43/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 59ms/step - accuracy: 0.9648 - loss: 0.0928 - val_accuracy: 0.9641 - val_loss: 0.0949\n",
      "Epoch 44/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 58ms/step - accuracy: 0.9651 - loss: 0.0917 - val_accuracy: 0.9634 - val_loss: 0.0969\n",
      "Epoch 45/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 60ms/step - accuracy: 0.9651 - loss: 0.0917 - val_accuracy: 0.9639 - val_loss: 0.0949\n",
      "Epoch 46/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 59ms/step - accuracy: 0.9652 - loss: 0.0915 - val_accuracy: 0.9638 - val_loss: 0.0958\n",
      "Epoch 47/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 61ms/step - accuracy: 0.9650 - loss: 0.0918 - val_accuracy: 0.9637 - val_loss: 0.0962\n",
      "Epoch 48/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 60ms/step - accuracy: 0.9650 - loss: 0.0921 - val_accuracy: 0.9643 - val_loss: 0.0943\n",
      "Epoch 49/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 62ms/step - accuracy: 0.9652 - loss: 0.0919 - val_accuracy: 0.9638 - val_loss: 0.0958\n",
      "Epoch 50/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 60ms/step - accuracy: 0.9650 - loss: 0.0918 - val_accuracy: 0.9630 - val_loss: 0.0976\n",
      "Epoch 51/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 60ms/step - accuracy: 0.9654 - loss: 0.0914 - val_accuracy: 0.9637 - val_loss: 0.0967\n",
      "Epoch 52/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 61ms/step - accuracy: 0.9654 - loss: 0.0916 - val_accuracy: 0.9646 - val_loss: 0.0935\n",
      "Epoch 53/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 61ms/step - accuracy: 0.9655 - loss: 0.0911 - val_accuracy: 0.9632 - val_loss: 0.0974\n",
      "Epoch 54/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 61ms/step - accuracy: 0.9654 - loss: 0.0915 - val_accuracy: 0.9641 - val_loss: 0.0956\n",
      "Epoch 55/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 60ms/step - accuracy: 0.9651 - loss: 0.0918 - val_accuracy: 0.9635 - val_loss: 0.0963\n",
      "Epoch 56/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 61ms/step - accuracy: 0.9655 - loss: 0.0909 - val_accuracy: 0.9639 - val_loss: 0.0959\n",
      "Epoch 57/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 59ms/step - accuracy: 0.9652 - loss: 0.0919 - val_accuracy: 0.9648 - val_loss: 0.0933\n",
      "Epoch 58/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 62ms/step - accuracy: 0.9655 - loss: 0.0909 - val_accuracy: 0.9643 - val_loss: 0.0947\n",
      "Epoch 59/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 60ms/step - accuracy: 0.9651 - loss: 0.0918 - val_accuracy: 0.9632 - val_loss: 0.0968\n",
      "Epoch 60/60\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 61ms/step - accuracy: 0.9653 - loss: 0.0913 - val_accuracy: 0.9640 - val_loss: 0.0953\n",
      "\n",
      "Finished training CNN model 9/9\n",
      "    n_total = 1.0e+07 samples\n",
      "         lr = 0.005\n",
      "Best test error rate was 3.5204% in epoch 57/60\n",
      "\n",
      "Total training time for 9 models: 386.32 mins\n"
     ]
    }
   ],
   "source": [
    "histories = {}\n",
    "t0_train = time.time()\n",
    "n_models = len(n_totals)*len(lrs)\n",
    "for i, (n_total, lr) in enumerate(itertools.product(n_totals, lrs)):\n",
    "  # train-test split\n",
    "  # grab first n_total samples from stim, and split them\n",
    "  # (Make sure the data type is np.int32 below, not idx_t!)\n",
    "  idxs_test, idxs_train = split_data(\n",
    "      np.arange(n_total, dtype=np.int32), \n",
    "      test_size=frac_test, \n",
    "      seed=12345, shuffle=False)\n",
    "  \n",
    "  # package stim output\n",
    "  data = {}\n",
    "  for label, index in zip([\"test\", \"train\"], \n",
    "                          [idxs_test, idxs_train]):\n",
    "      data[label] = [features_det_bits[index], \n",
    "                      features_det_evts[index],\n",
    "                      features_translation_map[index], \n",
    "                      features_final_det_evts[index] ]\n",
    "  \n",
    "  # train the model\n",
    "  print(f\"\\nTraining CNN model {i+1}/{n_models}\\n\"\n",
    "        f\"    n_total = {n_total:0.1e} samples\\n\" \n",
    "        f\"         lr = {lr}\\n\"\n",
    "        + \"-\"*28)\n",
    "  model_dxd = FullCNNModel(\n",
    "    observable_type, d, kernel_size, r,\n",
    "    [n_nodes for _ in range(2)], npol=2,\n",
    "    do_all_data_qubits=False, extended_kernel_output=True, \n",
    "    include_det_evts=True, include_last_kernel_dets=False, \n",
    "    include_last_dets=True, has_nonuniform_response=False, \n",
    "    use_translated_kernels=False\n",
    "  )\n",
    "  model_dxd.compile(\n",
    "     optimizer=Adam(learning_rate=lr), \n",
    "     loss='binary_crossentropy', metrics=['accuracy'])\n",
    "  history = model_dxd.fit(\n",
    "    x=data[\"train\"],\n",
    "    y=class_bits[idxs_train,:],\n",
    "    batch_size=batch_size,\n",
    "    epochs=n_epochs, \n",
    "    validation_data=(data[\"test\"], class_bits[idxs_test,:]),\n",
    "    callbacks=[\n",
    "      # tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5,\n",
    "      #                                 restore_best_weights=True),\n",
    "      # tf.keras.callbacks.LearningRateScheduler(learning_rate_scheduler)\n",
    "    ]\n",
    "  )\n",
    "\n",
    "  for metric, value in history.history.items():\n",
    "    histories[lr, n_total, metric] = value\n",
    "  \n",
    "  # evaluate the model\n",
    "  print(f\"\\nFinished training CNN model {i+1}/{n_models}\\n\"\n",
    "        f\"    n_total = {n_total:0.1e} samples\\n\" \n",
    "        f\"         lr = {lr}\")\n",
    "  best_epoch = np.argmax(history.history['val_accuracy'])\n",
    "  best_er_percent = 100*(1 - history.history['val_accuracy'][best_epoch])\n",
    "  print(f\"Best test error rate was {best_er_percent:.4f}% \"\n",
    "        f\"in epoch {best_epoch + 1}/{n_epochs}\")\n",
    "\n",
    "# save histories\n",
    "histories_file = f\"{base}/{run}/histories.csv\"\n",
    "histories = pd.DataFrame(histories)\n",
    "histories.columns.names = [\"lr\", \"N\", \"metric\"]\n",
    "histories.to_csv(histories_file, index=False) \n",
    "\n",
    "t1_train = time.time()\n",
    "dt = (t1_train - t0_train)/60\n",
    "print(f\"\\nTotal training time for {n_models} models: {dt:.2f} mins\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <th colspan=\"4\" halign=\"left\">0.050</th>\n",
       "      <th colspan=\"4\" halign=\"left\">0.005</th>\n",
       "      <th colspan=\"2\" halign=\"left\">0.050</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"2\" halign=\"left\">0.005</th>\n",
       "      <th colspan=\"4\" halign=\"left\">0.050</th>\n",
       "      <th colspan=\"4\" halign=\"left\">0.005</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N</th>\n",
       "      <th colspan=\"4\" halign=\"left\">100000</th>\n",
       "      <th colspan=\"4\" halign=\"left\">100000</th>\n",
       "      <th colspan=\"2\" halign=\"left\">1000000</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"2\" halign=\"left\">1000000</th>\n",
       "      <th colspan=\"4\" halign=\"left\">10000000</th>\n",
       "      <th colspan=\"4\" halign=\"left\">10000000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metric</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>loss</th>\n",
       "      <th>...</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.68085</td>\n",
       "      <td>1.981164</td>\n",
       "      <td>0.768613</td>\n",
       "      <td>0.534791</td>\n",
       "      <td>0.75165</td>\n",
       "      <td>0.529783</td>\n",
       "      <td>0.774212</td>\n",
       "      <td>0.458211</td>\n",
       "      <td>0.797620</td>\n",
       "      <td>0.481148</td>\n",
       "      <td>...</td>\n",
       "      <td>0.909376</td>\n",
       "      <td>0.214439</td>\n",
       "      <td>0.881876</td>\n",
       "      <td>0.274150</td>\n",
       "      <td>0.906478</td>\n",
       "      <td>0.217552</td>\n",
       "      <td>0.934171</td>\n",
       "      <td>0.157922</td>\n",
       "      <td>0.954423</td>\n",
       "      <td>0.116201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.77425</td>\n",
       "      <td>0.527309</td>\n",
       "      <td>0.768675</td>\n",
       "      <td>0.519174</td>\n",
       "      <td>0.78090</td>\n",
       "      <td>0.428267</td>\n",
       "      <td>0.783813</td>\n",
       "      <td>0.410250</td>\n",
       "      <td>0.854665</td>\n",
       "      <td>0.303015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.931899</td>\n",
       "      <td>0.164666</td>\n",
       "      <td>0.906966</td>\n",
       "      <td>0.219506</td>\n",
       "      <td>0.906498</td>\n",
       "      <td>0.218738</td>\n",
       "      <td>0.955200</td>\n",
       "      <td>0.114471</td>\n",
       "      <td>0.956563</td>\n",
       "      <td>0.111896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.77250</td>\n",
       "      <td>0.510266</td>\n",
       "      <td>0.768675</td>\n",
       "      <td>0.504232</td>\n",
       "      <td>0.80705</td>\n",
       "      <td>0.378435</td>\n",
       "      <td>0.826563</td>\n",
       "      <td>0.355975</td>\n",
       "      <td>0.885170</td>\n",
       "      <td>0.263749</td>\n",
       "      <td>...</td>\n",
       "      <td>0.941755</td>\n",
       "      <td>0.145418</td>\n",
       "      <td>0.906776</td>\n",
       "      <td>0.218943</td>\n",
       "      <td>0.906798</td>\n",
       "      <td>0.212177</td>\n",
       "      <td>0.957627</td>\n",
       "      <td>0.108985</td>\n",
       "      <td>0.958135</td>\n",
       "      <td>0.108934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "lr        0.050                                     0.005            \\\n",
       "N      100000                                    100000               \n",
       "metric accuracy      loss val_accuracy  val_loss accuracy      loss   \n",
       "0       0.68085  1.981164     0.768613  0.534791  0.75165  0.529783   \n",
       "1       0.77425  0.527309     0.768675  0.519174  0.78090  0.428267   \n",
       "2       0.77250  0.510266     0.768675  0.504232  0.80705  0.378435   \n",
       "\n",
       "lr                                0.050            ...        0.005            \\\n",
       "N                              1000000             ...     1000000              \n",
       "metric val_accuracy  val_loss  accuracy      loss  ... val_accuracy  val_loss   \n",
       "0          0.774212  0.458211  0.797620  0.481148  ...     0.909376  0.214439   \n",
       "1          0.783813  0.410250  0.854665  0.303015  ...     0.931899  0.164666   \n",
       "2          0.826563  0.355975  0.885170  0.263749  ...     0.941755  0.145418   \n",
       "\n",
       "lr         0.050                                      0.005            \\\n",
       "N       10000000                                   10000000             \n",
       "metric  accuracy      loss val_accuracy  val_loss  accuracy      loss   \n",
       "0       0.881876  0.274150     0.906478  0.217552  0.934171  0.157922   \n",
       "1       0.906966  0.219506     0.906498  0.218738  0.955200  0.114471   \n",
       "2       0.906776  0.218943     0.906798  0.212177  0.957627  0.108985   \n",
       "\n",
       "lr                             \n",
       "N                              \n",
       "metric val_accuracy  val_loss  \n",
       "0          0.954423  0.116201  \n",
       "1          0.956563  0.111896  \n",
       "2          0.958135  0.108934  \n",
       "\n",
       "[3 rows x 24 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "histories.head(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
